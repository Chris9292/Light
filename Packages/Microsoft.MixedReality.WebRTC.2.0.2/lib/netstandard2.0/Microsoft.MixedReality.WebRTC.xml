<?xml version="1.0"?>
<doc>
    <assembly>
        <name>Microsoft.MixedReality.WebRTC</name>
    </assembly>
    <members>
        <member name="T:Microsoft.MixedReality.WebRTC.AudioFrame">
            <summary>
            Single raw uncompressed audio frame.
            </summary>
            <remarks>
            The use of <c>ref struct</c> is an optimization to avoid heap allocation on each frame while
            having a nicer-to-use container to pass a frame accross methods.
            </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.AudioFrame.audioData">
            <summary>
            Buffer of audio samples for all channels.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.AudioFrame.bitsPerSample">
            <summary>
            Number of bits per sample, generally 8 or 16.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.AudioFrame.sampleRate">
            <summary>
            Sample rate, in Hz. Generally in the range 8-48 kHz.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.AudioFrame.channelCount">
            <summary>
            Number of audio channels.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.AudioFrame.sampleCount">
            <summary>
            Number of consecutive samples in the audio data buffer.
            WebRTC generally delivers frames in 10ms chunks, so for e.g. a 16 kHz
            sample rate the sample count would be 1000.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.AudioFrameDelegate">
            <summary>
            Delegate used for events when an audio frame has been produced
            and is ready for consumption.
            </summary>
            <param name="frame">The newly available audio frame.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer">
            <summary>
            High level interface for consuming WebRTC audio tracks.
            Enqueues audio frames for a <see cref="T:Microsoft.MixedReality.WebRTC.RemoteAudioTrack"/> in an internal buffer as they
            arrive. Users should call
            <see cref="M:Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.Read(System.Int32,System.Int32,System.Single[],System.Int32@,System.Boolean@,Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.PadBehavior)"/>
            to read samples from the buffer when needed.
            </summary>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.RemoteAudioTrack.CreateReadBuffer"/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.PadBehavior">
            <summary>
            Controls the padding behavior of
            <see cref="M:Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.Read(System.Int32,System.Int32,System.Single[],System.Int32@,System.Boolean@,Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.PadBehavior)"/>
            on underrun.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.PadBehavior.DoNotPad">
            <summary>
            Do not pad the samples array.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.PadBehavior.PadWithZero">
            <summary>
            Pad with zeros (silence).
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.PadBehavior.PadWithSine">
            <summary>
            Pad with a sine function.
            </summary>
            <remarks>
            Generates audible artifacts on underrun. Use for debugging.
            </remarks>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.Read(System.Int32,System.Int32,System.Single[],System.Int32@,System.Boolean@,Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.PadBehavior)">
             <summary>
             Fill <paramref name="samplesOut"/> with samples from the internal buffer.
             </summary>
             <remarks>
             This method reads the internal buffer starting from the oldest data.
             If the internal buffer is exhausted (underrun), <paramref name="samplesOut"/>
             is padded according to the value of <paramref name="padBehavior"/>.
            
             This method should be called regularly to consume the audio data as it is
             received. Note that the internal buffer can overrun (and some frames can be
             dropped) if this is not called frequently enough.
             </remarks>
             <param name="sampleRate">
             Desired sample rate. Data in the buffer is resampled if this is different from
             the native track rate.
             </param>
             <param name="numChannels">
             Desired number of channels. Should be 1 or 2. Data in the buffer is split/averaged
             if this is different from the native track channels number.
             </param>
             <param name="samplesOut">
             Will be filled with the samples read from the internal buffer. The function will
             try to fill the entire length of the array.
             </param>
             <param name="numSamplesRead">
             Set to the effective number of samples read.
             This will be generally equal to the length of <paramref name="samplesOut"/>, but can be less in
             case of underrun.
             </param>
             <param name="hasOverrun">
             Set to <c>true</c> if frames have been dropped from the internal
             buffer between the previous call to <c>Read</c> and this.
             </param>
             <param name="padBehavior">Controls how <paramref name="samplesOut"/> is padded in case of underrun.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.Read(System.Int32,System.Int32,System.Single[],Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.PadBehavior)">
            <summary>
            Fill <paramref name="samplesOut"/> with samples from the internal buffer.
            See <see cref="M:Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.Read(System.Int32,System.Int32,System.Single[],System.Int32@,System.Boolean@,Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.PadBehavior)"/>.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer.Dispose">
            <summary>
            Release the buffer.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.AudioTrackSource">
             <summary>
             Audio source for WebRTC audio tracks.
            
             The audio source is not bound to any peer connection, and can therefore be shared by multiple audio
             tracks from different peer connections. This is especially useful to share local audio capture devices
             (microphones) amongst multiple peer connections when building a multi-peer experience with a mesh topology
             (one connection per pair of peers).
            
             The user owns the audio track source, and is in charge of keeping it alive until after all tracks using it
             are destroyed, and then dispose of it. The behavior of disposing of the track source while a track is still
             using it is undefined. The <see cref="P:Microsoft.MixedReality.WebRTC.AudioTrackSource.Tracks"/> property contains the list of tracks currently using the
             source.
             </summary>
             <seealso cref="T:Microsoft.MixedReality.WebRTC.LocalAudioTrack"/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.AudioTrackSource.Name">
            <summary>
            A name for the audio track source, used for logging and debugging.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.AudioTrackSource.Tracks">
            <summary>
            List of local audio tracks this source is providing raw audio frames to.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.AudioTrackSource._nativeHandle">
            <summary>
            Handle to the native AudioTrackSource object.
            </summary>
            <remarks>
            In native land this is a <code>Microsoft::MixedReality::WebRTC::AudioTrackSourceHandle</code>.
            </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.AudioTrackSource._name">
            <summary>
            Backing field for <see cref="P:Microsoft.MixedReality.WebRTC.AudioTrackSource.Name"/>, and cache for the native name.
            Since the name can only be set by the user, this cached value is always up-to-date with the
            internal name of the native object, by design.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.AudioTrackSource._tracks">
            <summary>
            Backing field for <see cref="P:Microsoft.MixedReality.WebRTC.AudioTrackSource.Tracks"/>.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.AudioTrackSource.Dispose">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.AudioTrackSource.OnTrackAddedToSource(Microsoft.MixedReality.WebRTC.LocalAudioTrack)">
            <summary>
            Internal callback when a track starts using this source.
            </summary>
            <param name="track">The track using this source.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.AudioTrackSource.OnTrackRemovedFromSource(Microsoft.MixedReality.WebRTC.LocalAudioTrack)">
            <summary>
            Internal callback when a track stops using this source.
            </summary>
            <param name="track">The track not using this source anymore.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.AudioTrackSource.OnTracksRemovedFromSource(System.Collections.Generic.IEnumerable{Microsoft.MixedReality.WebRTC.LocalAudioTrack})">
            <summary>
            Internal callback when a list of tracks stop using this source, generally
            as a result of a peer connection owning said tracks being closed.
            </summary>
            <param name="tracks">The list of tracks not using this source anymore.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.AudioTrackSource.ToString">
            <inheritdoc/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.DataChannel">
             <summary>
             Encapsulates a data channel of a peer connection.
            
             A data channel is a "pipe" allowing to send and receive arbitrary data to the
             remote peer. Data channels are based on DTLS-SRTP, and are therefore secure (encrypted).
             Exact security guarantees are provided by the underlying WebRTC core implementation
             and the WebRTC standard itself.
            
             https://tools.ietf.org/wg/rtcweb/
             https://www.w3.org/TR/webrtc/
            
             An instance of <see cref="T:Microsoft.MixedReality.WebRTC.DataChannel"/> is created either by manually calling
             <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddDataChannelAsync(System.String,System.Boolean,System.Boolean,System.Threading.CancellationToken)"/>
             or one of its variants, or automatically by the implementation when a new data channel
             is created in-band by the remote peer (<see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelAdded"/>).
             <see cref="T:Microsoft.MixedReality.WebRTC.DataChannel"/> cannot be instantiated directly.
             </summary>
             <seealso cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddDataChannelAsync(System.String,System.Boolean,System.Boolean,System.Threading.CancellationToken)"/>
             <seealso cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddDataChannelAsync(System.UInt16,System.String,System.Boolean,System.Boolean,System.Threading.CancellationToken)"/>
             <seealso cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelAdded"/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.DataChannel.ChannelState">
            <summary>
            Connection state of a data channel.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.DataChannel.ChannelState.Connecting">
            <summary>
            The data channel has just been created, and negotiating is underway to establish
            a link between the peers. The data channel cannot be used to send/receive yet.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.DataChannel.ChannelState.Open">
            <summary>
            The data channel is open and ready to send and receive messages.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.DataChannel.ChannelState.Closing">
            <summary>
            The data channel is being closed, and is not available anymore for data exchange.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.DataChannel.ChannelState.Closed">
            <summary>
            The data channel reached end of life and can be destroyed.
            It cannot be re-connected; instead a new data channel must be created.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.DataChannel.BufferingChangedDelegate">
            <summary>
            Delegate for the <see cref="E:Microsoft.MixedReality.WebRTC.DataChannel.BufferingChanged"/> event.
            </summary>
            <param name="previous">Previous buffering size, in bytes.</param>
            <param name="current">New buffering size, in bytes.</param>
            <param name="limit">Maximum buffering size, in bytes.</param>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.DataChannel.PeerConnection">
            <summary>
            The <see cref="P:Microsoft.MixedReality.WebRTC.DataChannel.PeerConnection"/> object this data channel was created from and is attached to.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.DataChannel.ID">
            <summary>
            The unique identifier of the data channel in the current connection.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.DataChannel.Label">
            <summary>
            The data channel name in the current connection.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.DataChannel.Ordered">
            <summary>
            Indicates whether the data channel messages are ordered or not.
            Ordered messages are delivered in the order they are sent, at the cost of delaying later messages
            delivery to the application (via <see cref="E:Microsoft.MixedReality.WebRTC.DataChannel.MessageReceived"/>) when internally arriving out of order.
            </summary>
            <value><c>true</c> if messages are ordered.</value>
            <seealso cref="P:Microsoft.MixedReality.WebRTC.DataChannel.Reliable"/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.DataChannel.Reliable">
            <summary>
            Indicates whether the data channel messages are reliably delivered.
            Reliable messages are guaranteed to be delivered as long as the connection is not dropped.
            Unreliable messages may be silently dropped for whatever reason, and the implementation will
            not try to detect this nor resend them.
            </summary>
            <value><c>true</c> if messages are reliable.</value>
            <seealso cref="P:Microsoft.MixedReality.WebRTC.DataChannel.Ordered"/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.DataChannel.State">
            <summary>
            The channel connection state represents the connection status.
            Changes to this state are notified via the <see cref="E:Microsoft.MixedReality.WebRTC.DataChannel.StateChanged"/> event.
            </summary>
            <remarks>
            The code handling this event should unwind the stack before
            using any other MR-WebRTC APIs; re-entrancy is not supported.
            </remarks>
            <value>The channel connection state.</value>
            <seealso cref="E:Microsoft.MixedReality.WebRTC.DataChannel.StateChanged"/>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.DataChannel.StateChanged">
            <summary>
            Event triggered when the data channel state changes.
            The new state is available in <see cref="P:Microsoft.MixedReality.WebRTC.DataChannel.State"/>.
            </summary>
            <remarks>
            The code handling this event should unwind the stack before
            using any other MR-WebRTC APIs; re-entrancy is not supported.
            </remarks>
            <seealso cref="P:Microsoft.MixedReality.WebRTC.DataChannel.State"/>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.DataChannel.BufferingChanged">
            <summary>
            Event triggered when the data channel buffering changes. Users should monitor this to ensure
            calls to <see cref="M:Microsoft.MixedReality.WebRTC.DataChannel.SendMessage(System.Byte[])"/> do not fail. Internally the data channel contains
            a buffer of messages to send that could not be sent immediately, for example due to
            congestion control. Once this buffer is full, any further call to <see cref="M:Microsoft.MixedReality.WebRTC.DataChannel.SendMessage(System.Byte[])"/>
            will fail until some mesages are processed and removed to make space.
            </summary>
            <remarks>
            The code handling this event should unwind the stack before
            using any other MR-WebRTC APIs; re-entrancy is not supported.
            </remarks>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.DataChannel.SendMessage(System.Byte[])"/>.
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.DataChannel.MessageReceived">
            <summary>
            Event triggered when a message is received through the data channel.
            </summary>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.DataChannel.SendMessage(System.Byte[])"/>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.DataChannel.MessageReceivedUnsafe">
            <summary>
            Event fires when a message is received through the data channel.
            </summary>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.DataChannel.SendMessage(System.IntPtr,System.UInt64)"/>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.DataChannel._argsRef">
            <summary>
            Reference (GC handle) keeping the internal delegates alive while they are registered
            as callbacks with the native code.
            </summary>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.Interop.Utils.MakeWrapperRef(System.Object)"/>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.DataChannel._nativeHandle">
            <summary>
            Handle to the native DataChannel object.
            </summary>
            <remarks>
            In native land this is a <code>mrsDataChannelHandle</code>.
            </remarks>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DataChannel.DestroyNative">
            <summary>
            Dispose of the native data channel. Invoked by its owner (<see cref="P:Microsoft.MixedReality.WebRTC.DataChannel.PeerConnection"/>).
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DataChannel.SendMessage(System.Byte[])">
            <summary>
            Send a message through the data channel. If the message cannot be sent, for example because of congestion
            control, it is buffered internally. If this buffer gets full, an exception is thrown and this call is aborted.
            The internal buffering is monitored via the <see cref="E:Microsoft.MixedReality.WebRTC.DataChannel.BufferingChanged"/> event.
            </summary>
            <param name="message">The message to send to the remote peer.</param>
            <exception xref="System.InvalidOperationException">The native data channel is not initialized.</exception>
            <exception xref="System.Exception">The internal buffer is full.</exception>
            <exception cref="T:Microsoft.MixedReality.WebRTC.DataChannelNotOpenException">The data channel is not open yet.</exception>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.InitializeAsync(Microsoft.MixedReality.WebRTC.PeerConnectionConfiguration,System.Threading.CancellationToken)"/>
            <seealso cref="P:Microsoft.MixedReality.WebRTC.PeerConnection.Initialized"/>
            <seealso cref="E:Microsoft.MixedReality.WebRTC.DataChannel.BufferingChanged"/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DataChannel.SendMessage(System.IntPtr,System.UInt64)">
            <summary>
            Send a message through the data channel. If the message cannot be sent, for example because of congestion
            control, it is buffered internally. If this buffer gets full, an exception is thrown and this call is aborted.
            The internal buffering is monitored via the <see cref="E:Microsoft.MixedReality.WebRTC.DataChannel.BufferingChanged"/> event.
            </summary>
            <param name="message">The message to send to the remote peer.</param>
            <param name="size">The size of the message to send in octects.</param>
            <exception xref="InvalidOperationException">The native data channel is not initialized.</exception>
            <exception xref="Exception">The internal buffer is full.</exception>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.InitializeAsync(Microsoft.MixedReality.WebRTC.PeerConnectionConfiguration,System.Threading.CancellationToken)"/>
            <seealso cref="P:Microsoft.MixedReality.WebRTC.PeerConnection.Initialized"/>
            <seealso cref="E:Microsoft.MixedReality.WebRTC.DataChannel.BufferingChanged"/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.LocalAudioDeviceInitConfig">
            <summary>
            Configuration to initialize capture on a local audio device (microphone).
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.LocalAudioDeviceInitConfig.AutoGainControl">
            <summary>
            Enable automated gain control (AGC) on the audio device capture pipeline.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.DeviceAudioTrackSource">
            <summary>
            Implementation of an audio track source producing frames captured from an audio capture device (microphone).
            </summary>
            <seealso cref="T:Microsoft.MixedReality.WebRTC.LocalAudioTrack"/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DeviceAudioTrackSource.CreateAsync(Microsoft.MixedReality.WebRTC.LocalAudioDeviceInitConfig)">
            <summary>
            Create an audio track source using a local audio capture device (microphone).
            </summary>
            <param name="initConfig">Optional configuration to initialize the audio capture on the device.</param>
            <returns>The newly create audio track source.</returns>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.LocalAudioTrack.CreateFromSource(Microsoft.MixedReality.WebRTC.AudioTrackSource,Microsoft.MixedReality.WebRTC.LocalAudioTrackInitConfig)"/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DeviceAudioTrackSource.ToString">
            <inheritdoc/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig">
            <summary>
            Configuration to initialize capture on a local video device (webcam).
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.videoDevice">
            <summary>
            Optional video capture device to use for capture.
            Use the default device if not specified.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.videoProfileId">
            <summary>
            Optional unique identifier of the video profile to use for capture,
            if the device supports video profiles, as retrieved by one of:
            - <see xref="MediaCapture.FindAllVideoProfiles"/>
            - <see xref="MediaCapture.FindKnownVideoProfiles"/>
            This requires <see cref="F:Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.videoDevice"/> to be specified.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.videoProfileKind">
            <summary>
            Optional video profile kind to restrict the list of video profiles to consider.
            Note that this is not exclusive with <see cref="F:Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.videoProfileId"/>, although in
            practice it is recommended to specify only one or the other.
            This requires <see cref="F:Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.videoDevice"/> to be specified.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.enableMrc">
            <summary>
            Enable Mixed Reality Capture (MRC) on devices supporting the feature.
            This setting is silently ignored on device not supporting MRC.
            </summary>
            <remarks>
            This is only supported on UWP.
            </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.enableMrcRecordingIndicator">
            <summary>
            Display the on-screen recording indicator while MRC is enabled.
            This setting is silently ignored on device not supporting MRC, or if
            <see cref="F:Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.enableMrc"/> is set to <c>false</c>.
            </summary>
            <remarks>
            This is only supported on UWP.
            </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.width">
            <summary>
            Optional capture resolution width, in pixels.
            This must be a resolution width the device supports.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.height">
            <summary>
            Optional capture resolution height, in pixels.
            This must be a resolution width the device supports.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.framerate">
            <summary>
            Optional capture frame rate, in frames per second (FPS).
            This must be a capture framerate the device supports.
            </summary>
            <remarks>
            This is compared by strict equality, so is best left unspecified or to an exact value
            retrieved by <see cref="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureFormatsAsync(System.String)"/>.
            </remarks>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.VideoProfile">
            <summary>
            Video profile.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoProfile.uniqueId">
            <summary>
            Unique identifier of the video profile.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource">
            <summary>
            Implementation of a video track source producing frames captured from a video capture device (webcam).
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.FrameEncoding">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureDevicesAsync">
            <summary>
            Get the list of video capture devices available on the local host machine.
            </summary>
            <returns>The list of available video capture devices.</returns>
            <remarks>
            Assign one of the returned <see cref="T:Microsoft.MixedReality.WebRTC.VideoCaptureDevice"/> to the <see cref="F:Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig.videoDevice"/>
            field to force a local video track to use that device when creating it with
            <see cref="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.CreateAsync(Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig)"/>.
            </remarks>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureProfilesAsync(System.String)">
            <summary>
            Enumerate all the video profiles associated with the specified video capture device, if any.
            </summary>
            <param name="deviceId">Unique identifier of the video capture device to enumerate the
            capture formats of, as retrieved from the <see cref="F:Microsoft.MixedReality.WebRTC.VideoCaptureDevice.id"/> field of
            a capture device enumerated with <see cref="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureDevicesAsync"/>.</param>
            <returns>The list of available video profiles for the specified video capture device.</returns>
            <remarks>
            If the video capture device does not support video profiles, the function succeeds
            and returns an empty list.
            
            This is equivalent to:
            <code>
            GetCaptureProfilesAsync(deviceId, VideoProfileKind.Unspecified);
            </code>
            </remarks>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureProfilesAsync(System.String,Microsoft.MixedReality.WebRTC.VideoProfileKind)"/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureProfilesAsync(System.String,Microsoft.MixedReality.WebRTC.VideoProfileKind)">
            <summary>
            Enumerate the video profiles associated with the specified video capture device, if any,
            and restricted to the specified video profile kind.
            </summary>
            <param name="deviceId">Unique identifier of the video capture device to enumerate the
            capture formats of, as retrieved from the <see cref="F:Microsoft.MixedReality.WebRTC.VideoCaptureDevice.id"/> field of
            a capture device enumerated with <see cref="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureDevicesAsync"/>.</param>
            <param name="profileKind">Kind of video profile to enumerate. Specify
            <see cref="F:Microsoft.MixedReality.WebRTC.VideoProfileKind.Unspecified"/> to enumerate all profiles.</param>
            <returns>The list of available video profiles for the specified video capture device.</returns>
            <remarks>If the video capture device does not support video profiles, the function succeeds
            and returns an empty list.</remarks>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureProfilesAsync(System.String)"/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureFormatsAsync(System.String)">
            <summary>
            Enumerate the video capture formats for the specified video capture device.
            </summary>
            <param name="deviceId">Unique identifier of the video capture device to enumerate the
            capture formats of, as retrieved from the <see cref="F:Microsoft.MixedReality.WebRTC.VideoCaptureDevice.id"/> field of
            a capture device enumerated with <see cref="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureDevicesAsync"/>.</param>
            <returns>The list of available video capture formats for the specified video capture device.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureFormatsAsync(System.String,System.String)">
            <summary>
            Enumerate the video capture formats for the specified video capture device and video profile.
            </summary>
            <param name="deviceId">Unique identifier of the video capture device to enumerate the
            capture formats of, as retrieved from the <see cref="F:Microsoft.MixedReality.WebRTC.VideoCaptureDevice.id"/> field of
            a capture device enumerated with <see cref="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureDevicesAsync"/>.</param>
            <param name="profileId">Unique identifier of the video profile to enumerate the capture formats of,
            as retrieved from <see cref="F:Microsoft.MixedReality.WebRTC.VideoCaptureDevice.id"/> field of a capture device enumerated with
            <see cref="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureDevicesAsync"/>.</param>
            <returns>The list of available video capture formats for the specified video capture device.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureFormatsAsync(System.String,Microsoft.MixedReality.WebRTC.VideoProfileKind)">
            <summary>
            Enumerate the video capture formats for the specified video capture device and video profile.
            </summary>
            <param name="deviceId">Unique identifier of the video capture device to enumerate the
            capture formats of, as retrieved from the <see cref="F:Microsoft.MixedReality.WebRTC.VideoCaptureDevice.id"/> field of
            a capture device enumerated with <see cref="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureDevicesAsync"/>.</param>
            <param name="profileKind">Kind of video profile to enumerate the capture formats of.</param>
            <returns>The list of available video capture formats for the specified video capture device.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.CreateAsync(Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig)">
             <summary>
             Create a video track source using a local video capture device (webcam).
            
             The video track source produces raw video frames by capturing them from a capture device accessible
             from the local host machine, generally a USB webcam or built-in device camera. The video source
             initially starts in the capturing state, and will remain live for as long as the source is alive.
             Once the source is not live anymore (ended), it cannot be restarted. A new source must be created to
             use the same video capture device again.
            
             The source can be used to create one or more local video tracks (<see cref="T:Microsoft.MixedReality.WebRTC.LocalVideoTrack"/>), which
             once added to a video transceiver allow the video frames to be sent to a remote peer. The source itself
             is not associated with any peer connection, and can be used to create local video tracks from multiple
             peer connections at once, thereby being shared amongst those peer connections.
            
             The source is owned by the user, who must ensure it stays alive while being in use by at least one local
             video track. Once it is not used anymore, the user is in charge of disposing of the source. Disposing of
             a source still in use by a local video track is undefined behavior.
             </summary>
             <param name="initConfig">Optional configuration to initialize the video capture on the device.</param>
             <returns>The newly create video track source.</returns>
             <remarks>
             On UWP this requires the "webcam" capability.
             See <see href="https://docs.microsoft.com/en-us/windows/uwp/packaging/app-capability-declarations"/>
             for more details.
            
             The video capture device may be accessed several times during the initializing process,
             generally once for listing and validating the capture format, and once for actually starting
             the video capture. This is a limitation of the OS and/or hardware.
            
             Note that the capture device must support a capture format with the given constraints of profile
             ID or kind, capture resolution, and framerate, otherwise the call will fail. That is, there is no
             fallback mechanism selecting a closest match. Developers should use <see cref="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureFormatsAsync(System.String)"/>
             to list the supported formats ahead of calling <see cref="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.CreateAsync(Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig)"/>, and can
             build their own fallback mechanism on top of this call if needed.
             </remarks>
             <example>
             Create a video track source with Mixed Reality Capture (MRC) enabled.
             This assumes that the platform supports MRC. Note that if MRC is not available
             the call will still succeed, but will return a track without MRC enabled.
             <code>
             var initConfig = new LocalVideoDeviceInitConfig
             {
                 enableMrc = true
             };
             var videoSource = await VideoTrackSource.CreateFromDeviceAsync(initConfig);
             </code>
             Create a video track source from a local webcam, asking for a capture format suited for video conferencing,
             and a target framerate of 30 frames per second (FPS). The implementation will select an appropriate
             capture resolution. This assumes that the device supports video profiles, and has at least one capture
             format supporting exactly 30 FPS capture associated with the VideoConferencing profile. Otherwise the call
             will fail.
             <code>
             var initConfig = new LocalVideoDeviceInitConfig
             {
                 videoProfileKind = VideoProfileKind.VideoConferencing,
                 framerate = 30.0
             };
             var videoSource = await VideoTrackSource.CreateFromDeviceAsync(initConfig);
             </code>
             </example>
             <seealso cref="M:Microsoft.MixedReality.WebRTC.LocalVideoTrack.CreateFromSource(Microsoft.MixedReality.WebRTC.VideoTrackSource,Microsoft.MixedReality.WebRTC.LocalVideoTrackInitConfig)"/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.ToString">
            <inheritdoc/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.BufferTooSmallException">
            <summary>
            Exception raised when a buffer is too small to perform the current operation.
            
            Generally the buffer was provided by the caller, and this indicates that the caller
            must provide a larger buffer.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.BufferTooSmallException.#ctor">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.BufferTooSmallException.#ctor(System.String)">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.BufferTooSmallException.#ctor(System.String,System.Exception)">
            <inheritdoc/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.SctpNotNegotiatedException">
            <summary>
            Exception thrown when trying to add a data channel to a peer connection after
            a connection to a remote peer was established without an SCTP handshake.
            When using data channels, at least one data channel must be added to the peer
            connection before calling <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.CreateOffer"/> to signal
            to the implementation the intent to use data channels and the need to perform a
            SCTP handshake during the connection.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.SctpNotNegotiatedException.#ctor">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.SctpNotNegotiatedException.#ctor(System.String)">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.SctpNotNegotiatedException.#ctor(System.String,System.Exception)">
            <inheritdoc/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.InvalidInteropNativeHandleException">
            <summary>
            Exception thrown when an API function expects an interop handle to a valid native object,
            but receives an invalid handle instead.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.InvalidInteropNativeHandleException.#ctor">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.InvalidInteropNativeHandleException.#ctor(System.String)">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.InvalidInteropNativeHandleException.#ctor(System.String,System.Exception)">
            <inheritdoc/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.DataChannelNotOpenException">
             <summary>
             Exception thrown when trying to use a data channel that is not open.
            
             The user should listen to the <see cref="E:Microsoft.MixedReality.WebRTC.DataChannel.StateChanged"/> event until the
             <see cref="P:Microsoft.MixedReality.WebRTC.DataChannel.State"/> property is <see cref="F:Microsoft.MixedReality.WebRTC.DataChannel.ChannelState.Open"/>
             before trying to send some message with <see cref="M:Microsoft.MixedReality.WebRTC.DataChannel.SendMessage(System.Byte[])"/>.
             </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DataChannelNotOpenException.#ctor">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DataChannelNotOpenException.#ctor(System.String)">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DataChannelNotOpenException.#ctor(System.String,System.Exception)">
            <inheritdoc/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.FrameRequest">
            <summary>
            Request sent to an external video source via its registered callback to generate
            a new video frame for the track(s) connected to it.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.FrameRequest.Source">
            <summary>
            Video track source this request is associated with.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.FrameRequest.RequestId">
            <summary>
            Unique request identifier, for error checking.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.FrameRequest.TimestampMs">
            <summary>
            Frame timestamp, in milliseconds. This corresponds to the time when the request
            was made to the native video track source.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.FrameRequest.CompleteRequest(Microsoft.MixedReality.WebRTC.I420AVideoFrame@)">
            <summary>
            Complete the current request by providing a video frame for it.
            This must be used if the video track source was created with
            <see cref="M:Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.CreateFromI420ACallback(Microsoft.MixedReality.WebRTC.I420AVideoFrameRequestDelegate)"/>.
            </summary>
            <param name="frame">The video frame used to complete the request.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.FrameRequest.CompleteRequest(Microsoft.MixedReality.WebRTC.Argb32VideoFrame@)">
            <summary>
            Complete the current request by providing a video frame for it.
            This must be used if the video track source was created with
            <see cref="M:Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.CreateFromArgb32Callback(Microsoft.MixedReality.WebRTC.Argb32VideoFrameRequestDelegate)"/>.
            </summary>
            <param name="frame">The video frame used to complete the request.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.I420AVideoFrameRequestDelegate">
            <summary>
            Callback invoked when the WebRTC pipeline needs an external video source to generate
            a new video frame for the track(s) it is connected to.
            </summary>
            <param name="request">The request to fulfill with a new I420A video frame.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Argb32VideoFrameRequestDelegate">
            <summary>
            Callback invoked when the WebRTC pipeline needs an external video source to generate
            a new video frame for the track(s) it is connected to.
            </summary>
            <param name="request">The request to fulfill with a new ARGB32 video frame.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource">
             <summary>
             Video source for WebRTC video tracks based on a custom source
             of video frames managed by the user and external to the WebRTC
             implementation.
            
             This class is used to inject into the WebRTC engine a video track
             whose frames are produced by a user-managed source the WebRTC engine
             knows nothing about, like programmatically generated frames, including
             frames not strictly of video origin like a 3D rendered scene, or frames
             coming from a specific capture device not supported natively by WebRTC.
             This class serves as an adapter for such video frame sources.
             </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.FrameEncoding">
            <inheritdoc/>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource._frameRequestCallbackArgsHandle">
            <summary>
            GC handle to frame request callback args keeping the delegate alive
            while the callback is registered with the native implementation.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.CreateFromI420ACallback(Microsoft.MixedReality.WebRTC.I420AVideoFrameRequestDelegate)">
            <summary>
            Create a new external video track source from a given user callback providing I420A-encoded frames.
            </summary>
            <param name="frameCallback">The callback that will be used to request frames for tracks.</param>
            <returns>The newly created track source.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.CreateFromArgb32Callback(Microsoft.MixedReality.WebRTC.Argb32VideoFrameRequestDelegate)">
            <summary>
            Create a new external video track source from a given user callback providing ARGB32-encoded frames.
            </summary>
            <param name="frameCallback">The callback that will be used to request frames for tracks.</param>
            <returns>The newly created track source.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.CompleteFrameRequest(System.UInt32,System.Int64,Microsoft.MixedReality.WebRTC.I420AVideoFrame@)">
            <summary>
            Complete the current request by providing a video frame for it.
            This must be used if the video track source was created with
            <see cref="M:Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.CreateFromI420ACallback(Microsoft.MixedReality.WebRTC.I420AVideoFrameRequestDelegate)"/>.
            </summary>
            <param name="requestId">The original request ID.</param>
            <param name="timestampMs">The video frame timestamp.</param>
            <param name="frame">The video frame used to complete the request.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.CompleteFrameRequest(System.UInt32,System.Int64,Microsoft.MixedReality.WebRTC.Argb32VideoFrame@)">
            <summary>
            Complete the current request by providing a video frame for it.
            This must be used if the video track source was created with
            <see cref="M:Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.CreateFromArgb32Callback(Microsoft.MixedReality.WebRTC.Argb32VideoFrameRequestDelegate)"/>.
            </summary>
            <param name="requestId">The original request ID.</param>
            <param name="timestampMs">The video frame timestamp.</param>
            <param name="frame">The video frame used to complete the request.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.Dispose">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource.ToString">
            <inheritdoc/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.IAudioSource">
            <summary>
            Interface for audio sources, whether local sources/tracks or remote tracks.
            </summary>
            <seealso cref="T:Microsoft.MixedReality.WebRTC.AudioTrackSource"/>
            <seealso cref="T:Microsoft.MixedReality.WebRTC.LocalAudioTrack"/>
            <seealso cref="T:Microsoft.MixedReality.WebRTC.RemoteAudioTrack"/>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.IAudioSource.AudioFrameReady">
            <summary>
            Event that occurs when a new audio frame is available from the source, either
            because the source produced it locally (<see cref="T:Microsoft.MixedReality.WebRTC.AudioTrackSource"/>, <see cref="T:Microsoft.MixedReality.WebRTC.LocalAudioTrack"/>) or because
            it received it from the remote peer (<see cref="T:Microsoft.MixedReality.WebRTC.RemoteAudioTrack"/>).
            </summary>
            <remarks>
            WebRTC audio tracks produce an audio frame every 10 ms.
            If you want to process the audio frames as soon as they are received, without conversions,
            subscribe to <see cref="E:Microsoft.MixedReality.WebRTC.IAudioSource.AudioFrameReady"/>.
            If you want the audio frames to be buffered (and optionally resampled) automatically,
            and you want the application to control when new audio data is read, create an
            <see cref="T:Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer"/> using <see cref="M:Microsoft.MixedReality.WebRTC.IAudioSource.CreateReadBuffer"/>.
            </remarks>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.IAudioSource.Enabled">
            <summary>
            Enabled status of the source. If enabled, produces audio frames as expected. If
            disabled, produces silence instead.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.IAudioSource.CreateReadBuffer">
            <summary>
            Starts buffering the audio frames from in an <see cref="T:Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer"/>.
            </summary>
            <remarks>
            WebRTC audio tracks produce an audio frame every 10 ms.
            If you want the audio frames to be buffered (and optionally resampled) automatically,
            and you want the application to control when new audio data is read, create an
            <see cref="T:Microsoft.MixedReality.WebRTC.AudioTrackReadBuffer"/> using <see cref="M:Microsoft.MixedReality.WebRTC.IAudioSource.CreateReadBuffer"/>.
            If you want to process the audio frames as soon as they are received, without conversions,
            subscribe to <see cref="E:Microsoft.MixedReality.WebRTC.IAudioSource.AudioFrameReady"/> instead.
            </remarks>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.AudioTrackSourceHandle">
            <summary>
            Handle to a native audio track source object.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.DataChannelInterop.CallbackArgs">
            <summary>
            Utility to lock all data channel delegates registered with the native plugin and prevent their
            garbage collection while registered.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.DeviceAudioTrackSourceHandle">
            <summary>
            Handle to a native device audio track source object.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.DeviceAudioTrackSourceInterop.LocalAudioDeviceMarshalInitConfig">
            <summary>
            Marshaling struct for initializing settings when opening a local audio device.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Interop.DeviceAudioTrackSourceInterop.LocalAudioDeviceMarshalInitConfig.#ctor(Microsoft.MixedReality.WebRTC.LocalAudioDeviceInitConfig)">
            <summary>
            Constructor for creating a local audio device initialization settings marshaling struct.
            </summary>
            <param name="settings">The settings to initialize the newly created marshaling struct.</param>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.DeviceAudioTrackSource.CreateAsync(Microsoft.MixedReality.WebRTC.LocalAudioDeviceInitConfig)"/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceHandle">
            <summary>
            Handle to a native device video track source object.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.VideoCaptureDeviceMarshalInfo">
            <summary>
            Marshaling struct for enumerating a video capture device.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.VideoProfileMarshalInfo">
            <summary>
            Marshaling struct for enumerating a video profile.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.VideoCaptureFormatMarshalInfo">
            <summary>
            Marshaling struct for enumerating a video capture format.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig">
            <summary>
            Marshaling struct for initializing settings when opening a local video device.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.VideoDeviceId">
            <summary>
            Video capture device unique identifier, as returned by <see cref="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.GetCaptureDevicesAsync"/>.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.VideoProfileId">
            <summary>
            Optional video profile unique identifier to use.
            Ignored if the video capture device specified by <see cref="F:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.VideoDeviceId"/> does not
            support video profiles.
            </summary>
            <remarks>
            This is generally preferred over <see cref="F:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.VideoProfileKind"/> to get full
            control over the video profile selection. Specifying both this and <see cref="F:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.VideoProfileKind"/>
            is discouraged, as it over-constraints the selection algorithm.
            </remarks>
            <seealso xref="MediaCapture.IsVideoProfileSupported(string)"/>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.VideoProfileKind">
            <summary>
            Optional video profile kind to select a video profile from.
            Ignored if the video capture device specified by <see cref="F:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.VideoDeviceId"/> does not
            support video profiles.
            </summary>
            <remarks>
            This is generally preferred over <see cref="F:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.VideoProfileId"/> to find a matching
            capture format (resolution and/or framerate) when one does not care about which video
            profile provides this capture format. Specifying both this and <see cref="F:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.VideoProfileId"/>
            is discouraged, as it over-constraints the selection algorithm.
            </remarks>
            <seealso xref="MediaCapture.IsVideoProfileSupported(string)"/>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.Width">
            <summary>
            Optional capture resolution width, in pixels, or zero for no constraint.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.Height">
            <summary>
            Optional capture resolution height, in pixels, or zero for no constraint.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.Framerate">
            <summary>
            Optional capture framerate, in frames per second (FPS), or zero for no constraint.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.EnableMixedRealityCapture">
            <summary>
            Enable Mixed Reality Capture (MRC). This flag is ignored if the platform doesn't support MRC.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.EnableMRCRecordingIndicator">
            <summary>
            When MRC is enabled, enable the on-screen recording indicator.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Interop.DeviceVideoTrackSourceInterop.LocalVideoDeviceMarshalInitConfig.#ctor(Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig)">
            <summary>
            Constructor for creating a local video device initialization settings marshaling struct.
            </summary>
            <param name="settings">The settings to initialize the newly created marshaling struct.</param>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource.CreateAsync(Microsoft.MixedReality.WebRTC.LocalVideoDeviceInitConfig)"/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.ExternalVideoTrackSourceHandle">
            <summary>
            Handle to a native external video track source object.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.mrsBool">
            <summary>
            Interop boolean.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.mrsOptBool">
            <summary>
            Interop optional boolean, conceptually equivalent to <c>bool?</c>.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.MonoPInvokeCallbackAttribute">
             <summary>
             Attribute to decorate managed delegates used as native callbacks (reverse P/Invoke).
             Required by Mono in Ahead-Of-Time (AOT) compiling, and Unity with the IL2CPP backend.
             </summary>
            
             This attribute is required by Mono AOT and Unity IL2CPP, but not by .NET Core or Framework.
             The implementation was copied from the Mono source code (https://github.com/mono/mono).
             The type argument does not seem to be used anywhere in the code, and a stub implementation
             like this seems to be enough for IL2CPP to be able to marshal the delegate (untested on Mono).
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Interop.Utils.MemCpy(System.Void*,System.Void*,System.UInt64)">
            <summary>
            Unsafe utility to copy a contiguous block of memory.
            This is equivalent to the C function <c>memcpy()</c>, and is provided for optimization purpose only.
            </summary>
            <param name="dst">Pointer to the beginning of the destination buffer data is copied to.</param>
            <param name="src">Pointer to the beginning of the source buffer data is copied from.</param>
            <param name="size">Size of the memory block, in bytes.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Interop.Utils.MemCpyStride(System.Void*,System.Int32,System.Void*,System.Int32,System.Int32,System.Int32)">
             <summary>
             Unsafe utility to copy a memory block with stride.
            
             This utility loops over the rows of the input memory block, and copy them to the output
             memory block, then increment the read and write pointers by the source and destination
             strides, respectively. For each row, exactly <paramref name="elem_size"/> bytes are copied,
             even if the row stride is higher. The extra bytes in the destination buffer past the row
             size until the row stride are left untouched.
            
             This is equivalent to the following pseudo-code:
             <code>
             for (int row = 0; row &lt; elem_count; ++row) {
               memcpy(dst, src, elem_size);
               dst += dst_stride;
               src += src_stride;
             }
             </code>
             </summary>
             <param name="dst">Pointer to the beginning of the destination buffer data is copied to.</param>
             <param name="dst_stride">Stride in bytes of the destination rows. This must be greater than
             or equal to the row size <paramref name="elem_size"/>.</param>
             <param name="src">Pointer to the beginning of the source buffer data is copied from.</param>
             <param name="src_stride">Stride in bytes of the source rows. This must be greater than
             or equal to the row size <paramref name="elem_size"/>.</param>
             <param name="elem_size">Size of each row, in bytes.</param>
             <param name="elem_count">Total number of rows to copy.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Interop.Utils.GetExceptionForErrorCode(System.UInt32)">
            <summary>
            Helper to get an exception based on an error code.
            </summary>
            <param name="res">The error code to turn into an exception.</param>
            <returns>The exception corresponding to error code <paramref name="res"/>, or <c>null</c> if
            <paramref name="res"/> was <c>MRS_SUCCESS</c>.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Interop.Utils.ThrowOnErrorCode(System.UInt32)">
            <summary>
            Helper to throw an exception based on an error code.
            </summary>
            <param name="res">The error code to turn into an exception, if not zero (MRS_SUCCESS).</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Interop.Utils.SetFrameHeightRoundMode(Microsoft.MixedReality.WebRTC.PeerConnection.FrameHeightRoundMode)">
            <summary>
            See <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.SetFrameHeightRoundMode(Microsoft.MixedReality.WebRTC.PeerConnection.FrameHeightRoundMode)"/>.
            </summary>
            <param name="value"></param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.LocalAudioTrackHandle">
            <summary>
            Handle to a native local audio track object.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.LocalVideoTrackHandle">
            <summary>
            Handle to a native local video track object.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.ObjectHandle">
            <summary>
            Handle to a native object.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Interop.ObjectHandle.IsInvalid">
            <summary>
            Check if the current handle is invalid, which means it is not referencing
            an actual native object. Note that a valid handle only means that the internal
            handle references a native object, but does not guarantee that the native
            object is still accessible. It is only safe to access the native object if
            the handle is not closed, which implies it being valid.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Interop.ObjectHandle.#ctor">
            <summary>
            Default constructor for an invalid handle.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Interop.ObjectHandle.#ctor(System.IntPtr)">
            <summary>
            Constructor for a valid handle referencing the given native object.
            </summary>
            <param name="handle">The valid internal handle to the native object.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Interop.ObjectHandle.ReleaseHandle">
            <summary>
            Release the native object while the handle is being closed.
            </summary>
            <returns>Return <c>true</c> if the native object was successfully released.</returns>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionHandle">
            <summary>
            Handle to a native peer connection object.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.PeerCallbackArgs">
            <summary>
            Utility to lock all optional delegates registered with the native plugin for the duration
            of the peer connection wrapper lifetime, and prevent their garbage collection.
            </summary>
            <remarks>
            The delegate don't need to be pinned, just referenced to prevent garbage collection.
            So referencing them from this class is enough to keep them alive and usable.
            </remarks>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.DataChannelAddedInfo">
            <summary>
            Marshalling structure to receive information about a newly created data channel
            just added to the peer connection after a remote description was applied.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.DataChannelAddedInfo.dataChannelHandle">
            <summary>
            Handle of the newly created data channel.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.TransceiverAddedInfo">
            <summary>
            Marshalling structure to receive information about a newly created transceiver
            just added to the peer connection.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.TransceiverAddedInfo.transceiverHandle">
            <summary>
            Handle to the newly-created native transceiver object.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.TransceiverAddedInfo.name">
            <summary>
            Transceiver name.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.TransceiverAddedInfo.mediaKind">
            <summary>
            Kind of media the transceiver transports.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.TransceiverAddedInfo.mlineIndex">
            <summary>
            Media line index of the transceiver.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.TransceiverAddedInfo.encodedStreamIDs">
            <summary>
            Encoded string of semi-colon separated list of stream IDs.
            Example for stream IDs ("id1", "id2", "id3"):
              encodedStreamIDs = "id1;id2;id3";
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.TransceiverAddedInfo.desiredDirection">
            <summary>
            Initial desired direction of the transceiver on creation.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.RemoteAudioTrackAddedInfo">
            <summary>
            Marshalling structure to receive information about a newly created remote audio track
            just added to a transceiver of the peer connection.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.RemoteAudioTrackAddedInfo.trackHandle">
            <summary>
            Handle of the newly created remote audio track.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.RemoteAudioTrackAddedInfo.audioTransceiverHandle">
            <summary>
            Handle of the audio transceiver the track was added to.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.RemoteAudioTrackAddedInfo.trackName">
            <summary>
            Name of the remote audio track.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.RemoteVideoTrackAddedInfo">
            <summary>
            Marshalling structure to receive information about a newly created remote video track
            just added to a transceiver of the peer connection.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.RemoteVideoTrackAddedInfo.trackHandle">
            <summary>
            Handle of the newly created remote video track.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.RemoteVideoTrackAddedInfo.videoTransceiverHandle">
            <summary>
            Handle of the video transceiver the track was added to.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.PeerConnectionInterop.RemoteVideoTrackAddedInfo.trackName">
            <summary>
            Name of the remote video track.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.RefCountedObjectHandle">
            <summary>
            Handle to a native reference-counted object.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Interop.RefCountedObjectHandle.ReleaseHandle">
            <summary>
            Release the native object while the handle is being closed.
            </summary>
            <returns>Return <c>true</c> if the native object was successfully released.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Interop.RemoteAudioTrackInterop.ReadBufferHandle.#ctor">
            <summary>
            Used internally by <see cref="M:Microsoft.MixedReality.WebRTC.Interop.RemoteAudioTrackInterop.RemoteAudioTrack_CreateReadBuffer(Microsoft.MixedReality.WebRTC.Interop.RemoteAudioTrackInterop.RemoteAudioTrackHandle,Microsoft.MixedReality.WebRTC.Interop.RemoteAudioTrackInterop.ReadBufferHandle@)"/>.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.TransceiverInterop.InitConfig.name">
            <summary>
            Name of the transceiver, for logging and debugging.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.TransceiverInterop.InitConfig.mediaKind">
            <summary>
            Kind of media the new transceiver transports.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.TransceiverInterop.InitConfig.desiredDirection">
            <summary>
            Initial desired direction.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.TransceiverInterop.InitConfig.encodedStreamIds">
            <summary>
            Stream IDs of the transceiver, encoded as a semi-colon separated list of IDs.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Interop.VideoTrackSourceHandle">
            <summary>
            Handle to a native video track source object.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Interop.VideoTrackSourceInterop.I420AFrameCallback">
            Callback arguments to ensure delegates registered with the native layer don't go out of scope.
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.VideoEncoding">
            <summary>
            Enumeration of video encodings.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoEncoding.I420A">
            <summary>
            I420A video encoding with chroma (UV) halved in both directions (4:2:0),
            and optional Alpha plane.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoEncoding.Argb32">
            <summary>
            32-bit ARGB32 video encoding with 8-bit per component, encoded as uint32 little-endian
            0xAARRGGBB value, or equivalently (B,G,R,A) in byte order.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.IVideoSource">
            <summary>
            Interface for video sources, whether local or remote.
            </summary>
            <seealso cref="T:Microsoft.MixedReality.WebRTC.VideoTrackSource"/>
            <seealso cref="T:Microsoft.MixedReality.WebRTC.LocalVideoTrack"/>
            <seealso cref="T:Microsoft.MixedReality.WebRTC.RemoteVideoTrack"/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.IVideoSource.FrameEncoding">
            <summary>
            Video encoding indicating the kind of frames the source is producing.
            </summary>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.IVideoSource.I420AVideoFrameReady">
             <summary>
             Event that occurs when a new video frame is available from the source, either
             because the source produced it locally (<see cref="T:Microsoft.MixedReality.WebRTC.VideoTrackSource"/>, <see cref="T:Microsoft.MixedReality.WebRTC.LocalVideoTrack"/>) or because
             it received it from the remote peer (<see cref="T:Microsoft.MixedReality.WebRTC.RemoteVideoTrack"/>).
             </summary>
             <remarks>
             The event delivers to the handlers an I420-encoded video frame.
            
             This event is invoked on the WebRTC worker thread. Handlers can be added/removed safely
             while the event is invoked, but access to any resource used by its handlers must be
             synchronized manually. Note that a handler might be invoked (at most once) after it has
             been removed from the event.
             </remarks>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.IVideoSource.Argb32VideoFrameReady">
             <summary>
             Event that occurs when a new video frame is available from the source, either
             because the source produced it locally (<see cref="T:Microsoft.MixedReality.WebRTC.VideoTrackSource"/>, <see cref="T:Microsoft.MixedReality.WebRTC.LocalVideoTrack"/>) or because
             it received it from the remote peer (<see cref="T:Microsoft.MixedReality.WebRTC.RemoteVideoTrack"/>).
             </summary>
             <remarks>
             The event delivers to the handlers an ARGB32-encoded video frame.
            
             This event is invoked on the WebRTC worker thread. Handlers can be added/removed safely
             while the event is invoked, but access to any resource used by its handlers must be
             synchronized manually. Note that a handler might be invoked (at most once) after it has
             been removed from the event.
             </remarks>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.IVideoSource.Enabled">
            <summary>
            Enabled status of the source. If enabled, produces video frames as expected. If
            disabled, produces black frames instead.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Library">
            <summary>
            Container for library-wise global settings of MixedReality-WebRTC.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Library.ReportLiveObjects">
            <summary>
            Report all objects currently alive and tracked by the native implementation.
            This is a live report, which generally gets outdated as soon as the function
            returned, as new objects are created and others destroyed. Nonetheless this
            is may be helpful to diagnose issues with disposing objects.
            </summary>
            <returns>Returns the number of live objects at the time of the call.</returns>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Library.ShutdownOptionsFlags">
            <summary>
            Options for library shutdown.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Library.ShutdownOptionsFlags.None">
            <summary>
            Do nothing specific.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Library.ShutdownOptionsFlags.LogLiveObjects">
            <summary>
            Log with <see cref="M:Microsoft.MixedReality.WebRTC.Library.ReportLiveObjects"/> all objects still alive, to help debugging.
            This is recommended to prevent deadlocks during shutdown.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Library.ShutdownOptionsFlags.DebugBreakOnForceShutdown">
            <summary>
            When forcing shutdown, either because <see cref="M:Microsoft.MixedReality.WebRTC.Library.ForceShutdown"/> is called or
            because the program terminates, and some objects are still alive, attempt
            to break into the debugger. This is not available for all platforms.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Library.ShutdownOptionsFlags.Default">
            <summary>
            Default options.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Library.ShutdownOptions">
            <summary>
            Options used when shutting down the MixedReality-WebRTC library.
            disposed, to shutdown the internal threads and release the global resources, and allow the
            library's module to be unloaded.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Library.ForceShutdown">
            <summary>
            Forcefully shutdown the MixedReality-WebRTC library. This shall not be used under normal
            circumstances, but can be useful e.g. in the Unity editor when a test fails and proper
            clean-up is not ensured (in particular, disposing objects), to allow the shared module to
            shutdown and terminate its native threads, and be unloaded.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.LocalAudioTrackInitConfig">
            <summary>
            Settings for adding a local audio track backed by a local audio capture device (e.g. microphone).
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.LocalAudioTrackInitConfig.trackName">
            <summary>
            Name of the track to create, as used for the SDP negotiation.
            This name needs to comply with the requirements of an SDP token, as described in the SDP RFC
            https://tools.ietf.org/html/rfc4566#page-43. In particular the name cannot contain spaces nor
            double quotes <code>"</code>.
            The track name can optionally be empty, in which case the implementation will create a valid
            random track name.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.LocalAudioTrack">
            <summary>
            Audio track sending to the remote peer audio frames originating from
            a local track source (local microphone or other audio recording device).
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.LocalAudioTrack.Enabled">
            <summary>
            Enabled status of the track. If enabled, send local audio frames to the remote peer as
            expected. If disabled, send only black frames instead.
            </summary>
            <remarks>
            Reading the value of this property after the track has been disposed is valid, and returns
            <c>false</c>. Writing to this property after the track has been disposed throws an exception.
            </remarks>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.LocalAudioTrack.Source">
            <summary>
            Audio track source this track is pulling its audio frames from.
            </summary>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.LocalAudioTrack.AudioFrameReady">
            <inheritdoc/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.LocalAudioTrack._nativeHandle">
            <summary>
            Handle to the native LocalAudioTrack object.
            </summary>
            <remarks>
            In native land this is a <code>Microsoft::MixedReality::WebRTC::LocalAudioTrackHandle</code>.
            </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.LocalAudioTrack._selfHandle">
            <summary>
            Handle to self for interop callbacks. This adds a reference to the current object, preventing
            it from being garbage-collected.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.LocalAudioTrack._interopCallbackArgs">
            <summary>
            Callback arguments to ensure delegates registered with the native layer don't go out of scope.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.LocalAudioTrack.CreateFromSource(Microsoft.MixedReality.WebRTC.AudioTrackSource,Microsoft.MixedReality.WebRTC.LocalAudioTrackInitConfig)">
             <summary>
             Create an audio track from an existing audio track source.
            
             This does not add the track to any peer connection. Instead, the track must be added manually to
             an audio transceiver to be attached to a peer connection and transmitted to a remote peer.
             </summary>
             <param name="source">The track source which provides the raw audio frames to the newly created track.</param>
             <param name="initConfig">Configuration to initialize the track being created.</param>
             <returns>Asynchronous task completed once the track is created.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.LocalAudioTrack.Dispose">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.LocalAudioTrack.ToString">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.LocalAudioTrack.CreateReadBuffer">
            <inheritdoc/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.VideoProfileKind">
            <summary>
            Kind of video profile. This corresponds to the <see xref="Windows.Media.Capture.KnownVideoProfile"/>
            enum of the <see xref="Windows.Media.Capture.MediaCapture"/> API.
            </summary>
            <seealso href="https://docs.microsoft.com/en-us/uwp/api/windows.media.capture.knownvideoprofile"/>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoProfileKind.Unspecified">
            <summary>
            Unspecified video profile kind. Used to remove any constraint on the video profile kind.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoProfileKind.VideoRecording">
            <summary>
            Video profile for video recording, often of higher quality and framerate at the expense
            of power consumption and latency.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoProfileKind.HighQualityPhoto">
            <summary>
            Video profile for high quality photo capture.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoProfileKind.BalancedVideoAndPhoto">
            <summary>
            Balanced video profile to capture both videos and photos.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoProfileKind.VideoConferencing">
            <summary>
            Video profile for video conferencing, often of lower power consumption
            and lower latency by deprioritizing higher resolutions.
            This is the recommended profile for most WebRTC applications, if supported.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoProfileKind.PhotoSequence">
            <summary>
            Video profile for capturing a sequence of photos.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoProfileKind.HighFrameRate">
            <summary>
            Video profile containing high framerate capture formats.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoProfileKind.VariablePhotoSequence">
            <summary>
            Video profile for capturing a variable sequence of photos.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoProfileKind.HdrWithWcgVideo">
            <summary>
            Video profile for capturing videos with High Dynamic Range (HDR) and Wide Color Gamut (WCG).
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoProfileKind.HdrWithWcgPhoto">
            <summary>
            Video profile for capturing photos with High Dynamic Range (HDR) and Wide Color Gamut (WCG).
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoProfileKind.VideoHdr8">
            <summary>
            Video profile for capturing videos with High Dynamic Range (HDR).
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.LocalVideoTrackInitConfig">
            <summary>
            Settings for creating a new local video track.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.LocalVideoTrackInitConfig.trackName">
            <summary>
            Name of the track to create, as used for the SDP negotiation.
            This name needs to comply with the requirements of an SDP token, as described in the SDP RFC
            https://tools.ietf.org/html/rfc4566#page-43. In particular the name cannot contain spaces nor
            double quotes <code>"</code>.
            The track name can optionally be empty, in which case the implementation will create a valid
            random track name.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.LocalVideoTrack">
            <summary>
            Video track sending to the remote peer video frames originating from
            a local track source.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.LocalVideoTrack.Source">
            <summary>
            Video track source this track is pulling its video frames from.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.LocalVideoTrack.Enabled">
            <summary>
            Enabled status of the track. If enabled, send local video frames to the remote peer as
            expected. If disabled, send only black frames instead.
            </summary>
            <remarks>
            Reading the value of this property after the track has been disposed is valid, and returns
            <c>false</c>. Writing to this property after the track has been disposed throws an exception.
            </remarks>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.LocalVideoTrack.FrameEncoding">
            <inheritdoc/>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.LocalVideoTrack.I420AVideoFrameReady">
            <inheritdoc/>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.LocalVideoTrack.Argb32VideoFrameReady">
            <inheritdoc/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.LocalVideoTrack._nativeHandle">
            <summary>
            Handle to the native LocalVideoTrack object.
            </summary>
            <remarks>
            In native land this is a <code>Microsoft::MixedReality::WebRTC::LocalVideoTrackHandle</code>.
            </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.LocalVideoTrack._selfHandle">
            <summary>
            Handle to self for interop callbacks. This adds a reference to the current object, preventing
            it from being garbage-collected.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.LocalVideoTrack.CreateFromSource(Microsoft.MixedReality.WebRTC.VideoTrackSource,Microsoft.MixedReality.WebRTC.LocalVideoTrackInitConfig)">
             <summary>
             Create a video track from an existing video track source.
            
             This does not add the track to any peer connection. Instead, the track must be added manually to
             a video transceiver to be attached to a peer connection and transmitted to a remote peer.
             </summary>
             <param name="source">The track source which provides the raw video frames to the newly created track.</param>
             <param name="initConfig">Configuration to initialize the track being created.</param>
             <returns>Asynchronous task completed once the track is created.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.LocalVideoTrack.Dispose">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.LocalVideoTrack.ToString">
            <inheritdoc/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.MediaTrack">
            <summary>
            Base class for media tracks sending to or receiving from the remote peer.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.MediaTrack.Transceiver">
            <summary>
            Transceiver this track is attached to, if any.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.MediaTrack.PeerConnection">
            <summary>
            Peer connection this media track is added to, if any.
            This is <c>null</c> after the track has been removed from the peer connection.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.MediaTrack.Name">
            <summary>
            Track name as specified during creation. This property is immutable.
            For remote tracks the property is specified by the remote peer.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.MediaTrack.ToString">
            <inheritdoc/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.LocalMediaTrack">
            <summary>
            Base class for media tracks sending to the remote peer.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.LocalMediaTrack.Dispose">
            <summary>
            Remove the track from the associated <see cref="T:Microsoft.MixedReality.WebRTC.Transceiver"/> (if there is one)
            and release the corresponding resources.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.MovingAverage">
            <summary>
            Utility to manage a moving average of a time series.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.MovingAverage.Capacity">
            <summary>
            Number of samples in the moving average window.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.MovingAverage.Average">
            <summary>
            Average value of the samples.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.MovingAverage._samples">
            <summary>
            Queue of samples in the moving window.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.MovingAverage.#ctor(System.Int32)">
            <summary>
            Create a new moving average with a given window size.
            </summary>
            <param name="capacity">The capacity of the sample window.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.MovingAverage.Clear">
            <summary>
            Clear the moving average and discard all cached samples.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.MovingAverage.Push(System.Single)">
            <summary>
            Push a new sample and recalculate the current average.
            </summary>
            <param name="value">The new value to add.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.IceTransportType">
            <summary>
            Type of ICE candidates offered to the remote peer.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceTransportType.None">
            <summary>
            No ICE candidate offered.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceTransportType.Relay">
            <summary>
            Only advertize relay-type candidates, like TURN servers, to avoid leaking the IP address of the client.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceTransportType.NoHost">
            ?
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceTransportType.All">
            <summary>
            Offer all types of ICE candidates.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.BundlePolicy">
            <summary>
            Bundle policy.
            See https://www.w3.org/TR/webrtc/#rtcbundlepolicy-enum.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.BundlePolicy.Balanced">
            <summary>
            Gather ICE candidates for each media type in use (audio, video, and data). If the remote endpoint is
            not bundle-aware, negotiate only one audio and video track on separate transports.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.BundlePolicy.MaxBundle">
            <summary>
            Gather ICE candidates for only one track. If the remote endpoint is not bundle-aware, negotiate only
            one media track.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.BundlePolicy.MaxCompat">
            <summary>
            Gather ICE candidates for each track. If the remote endpoint is not bundle-aware, negotiate all media
            tracks on separate transports.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.SdpSemantic">
            <summary>
            SDP semantic used for (re)negotiating a peer connection.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.SdpSemantic.UnifiedPlan">
            <summary>
            Unified plan, as standardized in the WebRTC 1.0 standard.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.SdpSemantic.PlanB">
            <summary>
            Legacy Plan B, deprecated and soon removed.
            Only available for compatiblity with older implementations if needed.
            Do not use unless there is a problem with the Unified Plan.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.IceServer">
            <summary>
            ICE server configuration (STUN and/or TURN).
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceServer.Urls">
             <summary>
             List of TURN and/or STUN server URLs to use for NAT bypass, in order of preference.
            
             The scheme is defined in the core WebRTC implementation, and is in short:
             stunURI     = stunScheme ":" stun-host [ ":" stun-port ]
             stunScheme  = "stun" / "stuns"
             turnURI     = turnScheme ":" turn-host [ ":" turn-port ] [ "?transport=" transport ]
             turnScheme  = "turn" / "turns"
             </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceServer.TurnUserName">
            <summary>
            Optional TURN server username.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceServer.TurnPassword">
            <summary>
            Optional TURN server credentials.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.IceServer.ToString">
            <summary>
            Format the ICE server data according to the encoded marshalling of the C++ API.
            </summary>
            <returns>The encoded string of ICE servers.</returns>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnectionConfiguration">
            <summary>
            Configuration to initialize a <see cref="T:Microsoft.MixedReality.WebRTC.PeerConnection"/>.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnectionConfiguration.IceServers">
            <summary>
            List of TURN and/or STUN servers to use for NAT bypass, in order of preference.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnectionConfiguration.IceTransportType">
            <summary>
            ICE transport policy for the connection.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnectionConfiguration.BundlePolicy">
            <summary>
            Bundle policy for the connection.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnectionConfiguration.SdpSemantic">
            <summary>
            SDP semantic for the connection.
            </summary>
            <remarks>Plan B is deprecated, do not use it.</remarks>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.IceConnectionState">
            <summary>
            State of an ICE connection.
            </summary>
            <remarks>
            Due to the underlying implementation, this is currently a mix of the
            <see href="https://www.w3.org/TR/webrtc/#rtcicegatheringstate-enum">RTPIceGatheringState</see>
            and the <see href="https://www.w3.org/TR/webrtc/#rtcpeerconnectionstate-enum">RTPPeerConnectionState</see>
            from the WebRTC 1.0 standard.
            </remarks>
            <seealso href="https://www.w3.org/TR/webrtc/#rtcicegatheringstate-enum"/>
            <seealso href="https://www.w3.org/TR/webrtc/#rtcpeerconnectionstate-enum"/>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceConnectionState.New">
            <summary>
            Newly created ICE connection. This is the starting state.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceConnectionState.Checking">
            <summary>
            ICE connection received an offer, but transports are not writable yet.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceConnectionState.Connected">
            <summary>
            Transports are writable.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceConnectionState.Completed">
            <summary>
            ICE connection finished establishing.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceConnectionState.Failed">
            <summary>
            Failed establishing an ICE connection.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceConnectionState.Disconnected">
            <summary>
            ICE connection is disconnected, there is no more writable transport.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceConnectionState.Closed">
            <summary>
            The peer connection was closed entirely.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.IceGatheringState">
            <summary>
            State of an ICE gathering process.
            </summary>
            <remarks>
            See <see href="https://www.w3.org/TR/webrtc/#rtcicegatheringstate-enum">RTPIceGatheringState</see>
            from the WebRTC 1.0 standard.
            </remarks>
            <seealso href="https://www.w3.org/TR/webrtc/#rtcicegatheringstate-enum"/>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceGatheringState.New">
            <summary>
            There is no ICE transport, or none of them started gathering ICE candidates.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceGatheringState.Gathering">
            <summary>
            The gathering process started. At least one ICE transport is active and gathering
            some ICE candidates.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceGatheringState.Complete">
            <summary>
            The gathering process is complete. At least one ICE transport was active, and
            all transports finished gathering ICE candidates.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.VideoCaptureDevice">
            <summary>
            Identifier for a video capture device.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoCaptureDevice.id">
            <summary>
            Unique device identifier.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoCaptureDevice.name">
            <summary>
            Friendly device name.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.VideoCaptureFormat">
            <summary>
            Capture format for a video track.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoCaptureFormat.width">
            <summary>
            Frame width, in pixels.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoCaptureFormat.height">
            <summary>
            Frame height, in pixels.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoCaptureFormat.framerate">
            <summary>
            Capture framerate, in frames per second.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoCaptureFormat.fourcc">
            <summary>
            FOURCC identifier of the video encoding.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.TransceiverInitSettings">
            <summary>
            Settings to create a new transceiver wrapper.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.TransceiverInitSettings.Name">
            <summary>
            Transceiver name, for logging and debugging.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.TransceiverInitSettings.InitialDesiredDirection">
            <summary>
            Initial value of <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.DesiredDirection"/>.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.TransceiverInitSettings.StreamIDs">
            <summary>
            List of stream IDs to associate the transceiver with.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.DelayedEvent">
            <summary>
            Wrapper for an event possibly delayed.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.DelayedEvent.Event">
            <summary>
            The event handler.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DelayedEvent.BeginSuspend">
            <summary>
            Begin suspending <see cref="P:Microsoft.MixedReality.WebRTC.DelayedEvent.Event"/>. This must be matched with a call
            to <see cref="M:Microsoft.MixedReality.WebRTC.DelayedEvent.EndSuspend"/>. During this time, calling <see cref="M:Microsoft.MixedReality.WebRTC.DelayedEvent.Invoke(System.Boolean)"/>
            does not invoke the event but instead queue it for later invoking by
            <see cref="M:Microsoft.MixedReality.WebRTC.DelayedEvent.EndSuspend"/>.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DelayedEvent.EndSuspend">
            <summary>
            End suspending <see cref="P:Microsoft.MixedReality.WebRTC.DelayedEvent.Event"/> and invoke it if any call to <see cref="M:Microsoft.MixedReality.WebRTC.DelayedEvent.Invoke(System.Boolean)"/>
            was made since the first <see cref="M:Microsoft.MixedReality.WebRTC.DelayedEvent.BeginSuspend"/> call.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DelayedEvent.Invoke(System.Boolean)">
            <summary>
            Try to invoke <see cref="P:Microsoft.MixedReality.WebRTC.DelayedEvent.Event"/>, either immediately if not suspended, or later
            when the last <see cref="M:Microsoft.MixedReality.WebRTC.DelayedEvent.EndSuspend"/> call stops suspending it.
            </summary>
            <param name="async">If the event is not suspended, and therefore is invoked immediately,
            then invoke it asynchronously from a worker thread.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.DelayedEvent.InvokeAsync">
            <summary>
            Try to invoke <see cref="P:Microsoft.MixedReality.WebRTC.DelayedEvent.Event"/> asynchronously.
            This is equivalent to <code>Invoke(async: true)</code>.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.DelayedEvent._lock">
            <summary>
            Lock for internal variables:
            - <see cref="F:Microsoft.MixedReality.WebRTC.DelayedEvent._suspendCount"/>
            - <see cref="F:Microsoft.MixedReality.WebRTC.DelayedEvent._eventPending"/>
            - <see cref="F:Microsoft.MixedReality.WebRTC.DelayedEvent._event"/>
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.DelayedEvent._suspendCount">
            <summary>
            Number of concurrent calls currently suspending the event.
            When this value reaches zero, the thread which decremented it checks the value of
            <see cref="F:Microsoft.MixedReality.WebRTC.DelayedEvent._eventPending"/> and if <c>true</c> then invoke the event.
            </summary>
            <remarks>This is protected by <see cref="F:Microsoft.MixedReality.WebRTC.DelayedEvent._lock"/>.</remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.DelayedEvent._eventPending">
            <summary>
            Was any event internally raised while the public event was suspended (that is, <see cref="F:Microsoft.MixedReality.WebRTC.DelayedEvent._suspendCount"/>
            was non-zero)? This is set by <see cref="M:Microsoft.MixedReality.WebRTC.DelayedEvent.Invoke(System.Boolean)"/> and cleared by any thread actually invoking the event after
            decrementing <see cref="F:Microsoft.MixedReality.WebRTC.DelayedEvent._suspendCount"/> to zero.
            </summary>
            <remarks>This is protected by <see cref="F:Microsoft.MixedReality.WebRTC.DelayedEvent._lock"/>.</remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.DelayedEvent._event">
            <summary>
            Backup field for <see cref="P:Microsoft.MixedReality.WebRTC.DelayedEvent.Event"/>, to be accessed only under <see cref="F:Microsoft.MixedReality.WebRTC.DelayedEvent._lock"/>.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.ScopedDelayedEvent">
            <summary>
            RAII helper to start/stop a delay block for a <see cref="T:Microsoft.MixedReality.WebRTC.DelayedEvent"/>.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.ScopedDelayedEvent.#ctor(Microsoft.MixedReality.WebRTC.DelayedEvent)">
            <summary>
            Initialize a scoped delay for the specified <see cref="T:Microsoft.MixedReality.WebRTC.DelayedEvent"/>.
            This will call <see cref="M:Microsoft.MixedReality.WebRTC.DelayedEvent.BeginSuspend"/> immediately, and will call
            <see cref="M:Microsoft.MixedReality.WebRTC.DelayedEvent.EndSuspend"/> once disposed.
            </summary>
            <param name="ev"></param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.ScopedDelayedEvent.Dispose">
            <summary>
            Dispose of the helper and call <see cref="M:Microsoft.MixedReality.WebRTC.DelayedEvent.BeginSuspend"/> on
            <see cref="F:Microsoft.MixedReality.WebRTC.ScopedDelayedEvent._delayedEvent"/>.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.SdpMessageType">
            <summary>
            Type of SDP message.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.SdpMessageType.Offer">
            <summary>
            Offer message used to initiate a new session.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.SdpMessageType.Answer">
            <summary>
            Answer message used to accept a session offer.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.SdpMessage">
            <summary>
            SDP message passed between the local and remote peers via the user's signaling solution.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.SdpMessage.Type">
            <summary>
            The message type.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.SdpMessage.Content">
            <summary>
            The raw message content.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.SdpMessage.TypeToString(Microsoft.MixedReality.WebRTC.SdpMessageType)">
            <summary>
            Convert an SDP message type to its internal string representation.
            </summary>
            <param name="type">The SDP message type to convert</param>
            <returns>The string representation of the SDP message type</returns>
            <exception xref="ArgumentException">The SDP message type was invalid.</exception>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.SdpMessage.StringToType(System.String)">
            <summary>
            Convert an internal string representation of an SDP message type back to its enumerated value.
            </summary>
            <param name="type">The internal string representation of the SDP message</param>
            <returns>The SDP message type associated with the string representation</returns>
            <exception xref="ArgumentException">The string does not represent any SDP message type.</exception>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.IceCandidate">
            <summary>
            ICE candidate to send to a remote peer or received from it.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceCandidate.SdpMid">
            <summary>
            Media ID (m=) of the candidate.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceCandidate.SdpMlineIndex">
            <summary>
            Index of the media line associated with the candidate.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.IceCandidate.Content">
            <summary>
            Candidate raw content.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection">
            <summary>
            The WebRTC peer connection object is the entry point to using WebRTC.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.TransceiverAddedDelegate">
            <summary>
            Delegate for <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.TransceiverAdded"/> event.
            </summary>
            <param name="transceiver">The newly added transceiver.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.AudioTrackAddedDelegate">
            <summary>
            Delegate for <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.AudioTrackAdded"/> event.
            </summary>
            <param name="track">The newly added audio track.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.AudioTrackRemovedDelegate">
            <summary>
            Delegate for <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.AudioTrackRemoved"/> event.
            </summary>
            <param name="transceiver">The audio transceiver the track was removed from.</param>
            <param name="track">The audio track just removed.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.VideoTrackAddedDelegate">
            <summary>
            Delegate for <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.VideoTrackAdded"/> event.
            </summary>
            <param name="track">The newly added video track.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.VideoTrackRemovedDelegate">
            <summary>
            Delegate for <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.VideoTrackRemoved"/> event.
            </summary>
            <param name="transceiver">The video transceiver the track was removed from.</param>
            <param name="track">The video track just removed.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelAddedDelegate">
            <summary>
            Delegate for <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelAdded"/> event.
            </summary>
            <param name="channel">The newly added data channel.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelRemovedDelegate">
            <summary>
            Delegate for <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelRemoved"/> event.
            </summary>
            <param name="channel">The data channel just removed.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.LocalSdpReadyToSendDelegate">
            <summary>
            Delegate for <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.LocalSdpReadytoSend"/> event.
            </summary>
            <param name="message">SDP message to send.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.IceCandidateReadytoSendDelegate">
            <summary>
            Delegate for the <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.IceCandidateReadytoSend"/> event.
            </summary>
            <param name="candidate">The ICE candidate to send.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.IceStateChangedDelegate">
            <summary>
            Delegate for the <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.IceStateChanged"/> event.
            </summary>
            <param name="newState">The new ICE connection state.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.IceGatheringStateChangedDelegate">
            <summary>
            Delegate for the <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.IceGatheringStateChanged"/> event.
            </summary>
            <param name="newState">The new ICE gathering state.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.TrackKind">
            <summary>
            Kind of WebRTC track.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.TrackKind.Unknown">
            <summary>
            Unknown track kind. Generally not initialized or error.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.TrackKind.Audio">
            <summary>
            Audio track.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.TrackKind.Video">
            <summary>
            Video track.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.TrackKind.Data">
            <summary>
            Data track.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.PreferredAudioCodec">
            <summary>
            Name of the preferred audio codec, or empty to let WebRTC decide.
            See https://en.wikipedia.org/wiki/RTP_audio_video_profile for the standard SDP names.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.PreferredAudioCodecExtraParamsRemote">
             <summary>
             Advanced use only. List of additional codec-specific arguments requested to the
             remote endpoint.
             </summary>
             <remarks>
             This must be a semicolon-separated list of "key=value" pairs. Arguments are passed as is,
             and there is no check on the validity of the parameter names nor their value.
             Arguments are added to the audio codec section of SDP messages sent to the remote endpoint.
            
             This is ignored if <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection.PreferredAudioCodec"/> is an empty string, or is not
             a valid codec name found in the SDP message offer.
             </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.PreferredAudioCodecExtraParamsLocal">
             <summary>
             Advanced use only. List of additional codec-specific arguments set on the local endpoint.
             </summary>
             <remarks>
             This must be a semicolon-separated list of "key=value" pairs. Arguments are passed as is,
             and there is no check on the validity of the parameter names nor their value.
             Arguments are set locally by adding them to the audio codec section of SDP messages
             received from the remote endpoint.
            
             This is ignored if <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection.PreferredAudioCodec"/> is an empty string, or is not
             a valid codec name found in the SDP message offer.
             </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.PreferredVideoCodec">
            <summary>
            Name of the preferred video codec, or empty to let WebRTC decide.
            See https://en.wikipedia.org/wiki/RTP_audio_video_profile for the standard SDP names.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.PreferredVideoCodecExtraParamsRemote">
             <summary>
             Advanced use only. List of additional codec-specific arguments requested to the
             remote endpoint.
             </summary>
             <remarks>
             This must be a semicolon-separated list of "key=value" pairs. Arguments are passed as is,
             and there is no check on the validity of the parameter names nor their value.
             Arguments are added to the video codec section of SDP messages sent to the remote endpoint.
            
             This is ignored if <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection.PreferredVideoCodec"/> is an empty string, or is not
             a valid codec name found in the SDP message offer.
             </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.PreferredVideoCodecExtraParamsLocal">
             <summary>
             Advanced use only. List of additional codec-specific arguments set on the local endpoint.
             </summary>
             <remarks>
             This must be a semicolon-separated list of "key=value" pairs. Arguments are passed as is,
             and there is no check on the validity of the parameter names nor their value.
             Arguments are set locally by adding them to the video codec section of SDP messages
             received from the remote endpoint.
            
             This is ignored if <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection.PreferredVideoCodec"/> is an empty string, or is not
             a valid codec name found in the SDP message offer.
             </remarks>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.PeerConnection.Name">
            <summary>
            A name for the peer connection, used for logging and debugging.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.PeerConnection.Initialized">
            <summary>
            Boolean property indicating whether the peer connection has been initialized.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.PeerConnection.IsConnected">
            <summary>
            Indicates whether the peer connection is established and can exchange some
            track content (audio/video/data) with the remote peer.
            </summary>
            <remarks>
            This does not indicate whether the ICE exchange is done, as it
            may continue after the peer connection negotiated a first session.
            For ICE connection status, see the <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.IceStateChanged"/> event.
            </remarks>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.PeerConnection.Transceivers">
            <summary>
            Collection of transceivers for the peer connection. Once a transceiver is added
            to the peer connection, it cannot be removed, but its tracks can be changed.
            Adding a transceiver or changing its direction require some new session negotiation.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.PeerConnection.AssociatedTransceivers">
             <summary>
             Collection of transceivers which have already been associated with a media line.
            
             A transceiver is associated with a media line when a local or remote offer is applied
             to the peer connection, respectively during <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.CreateOffer"/> and
             <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.SetRemoteDescriptionAsync(Microsoft.MixedReality.WebRTC.SdpMessage)"/>.
             </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.PeerConnection.LocalAudioTracks">
            <summary>
            Collection of local audio tracks attached to the peer connection.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.PeerConnection.LocalVideoTracks">
            <summary>
            Collection of local video tracks attached to the peer connection.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.PeerConnection.RemoteAudioTracks">
            <summary>
            Collection of remote audio tracks attached to the peer connection.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.PeerConnection.RemoteVideoTracks">
            <summary>
            Collection of remote video tracks attached to the peer connection.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannels">
             <summary>
             Collection of data channels for the peer connection.
            
             Data channels are either manually added by calling
             <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddDataChannelAsync(System.String,System.Boolean,System.Boolean,System.Threading.CancellationToken)"/> or
             <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddDataChannelAsync(System.UInt16,System.String,System.Boolean,System.Boolean,System.Threading.CancellationToken)"/>,
             or are created by the implementation while applying a remote offer when the remote
             peer created a new in-band data channel.
             </summary>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.PeerConnection.Connected">
            <summary>
            Event fired when a connection is established.
            </summary>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelAdded">
            <summary>
            Event fired when a data channel is added to the peer connection.
            This event is always fired, whether the data channel is created by the local peer
            or the remote peer, and is negotiated (out-of-band) or not (in-band).
            If an in-band data channel is created by the local peer, the <see cref="P:Microsoft.MixedReality.WebRTC.DataChannel.ID"/>
            field is not yet available when this event is fired, because the ID has not been
            agreed upon with the remote peer yet.
            </summary>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelRemoved">
            <summary>
            Event fired when a data channel is removed from the peer connection.
            This event is always fired, whatever its creation method (negotiated or not)
            and original creator (local or remote peer).
            </summary>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.PeerConnection.LocalSdpReadytoSend">
            <summary>
            Event that occurs when a local SDP message is ready to be transmitted.
            </summary>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.PeerConnection.IceCandidateReadytoSend">
            <summary>
            Event that occurs when a local ICE candidate is ready to be transmitted.
            </summary>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.PeerConnection.IceStateChanged">
            <summary>
            Event that occurs when the state of the ICE connection changed.
            </summary>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.PeerConnection.IceGatheringStateChanged">
            <summary>
            Event that occurs when the state of the ICE gathering changed.
            </summary>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.PeerConnection.RenegotiationNeeded">
            <summary>
            Event that occurs when a renegotiation of the session is needed.
            This generally occurs as a result of adding or removing tracks,
            and the user should call <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.CreateOffer"/> to actually
            start a renegotiation.
            </summary>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.PeerConnection.TransceiverAdded">
            <summary>
            Event that occurs when a transceiver is added to the peer connection, either
            manually using <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddTransceiver(Microsoft.MixedReality.WebRTC.MediaKind,Microsoft.MixedReality.WebRTC.TransceiverInitSettings)"/>, or
            automatically as a result of a new session negotiation.
            </summary>
            <remarks>
            Transceivers cannot be removed from the peer connection, so there is no
            <c>TransceiverRemoved</c> event.
            </remarks>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.PeerConnection.AudioTrackAdded">
            <summary>
            Event that occurs when a remote audio track is added to the current connection.
            </summary>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.PeerConnection.AudioTrackRemoved">
            <summary>
            Event that occurs when a remote audio track is removed from the current connection.
            </summary>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.PeerConnection.VideoTrackAdded">
            <summary>
            Event that occurs when a remote video track is added to the current connection.
            </summary>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.PeerConnection.VideoTrackRemoved">
            <summary>
            Event that occurs when a remote video track is removed from the current connection.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection._selfHandle">
            <summary>
            GCHandle to self for the various native callbacks.
            This also acts as a marker of a connection created or in the process of being created.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection._nativePeerhandle">
            <summary>
            Handle to the native PeerConnection object.
            </summary>
            <remarks>
            In native land this is a <code>Microsoft::MixedReality::WebRTC::PeerConnectionHandle</code>.
            </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection._initTask">
            <summary>
            Initialization task returned by <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.InitializeAsync(Microsoft.MixedReality.WebRTC.PeerConnectionConfiguration,System.Threading.CancellationToken)"/>.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection._isClosing">
            <summary>
            Boolean to indicate if <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.Close"/> has been called and is waiting for a pending
            initializing task <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection._initTask"/> to complete or cancel.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection._openCloseLock">
            <summary>
            Lock for asynchronous opening and closing of the connection, protecting
            changes to <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection._nativePeerhandle"/>, <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection._selfHandle"/>,
            <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection._initTask"/>, and <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection._isClosing"/>.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection._tracksLock">
            <summary>
            Lock for accessing the collections of tracks and transceivers:
            - <see cref="P:Microsoft.MixedReality.WebRTC.PeerConnection.Transceivers"/>
            - <see cref="P:Microsoft.MixedReality.WebRTC.PeerConnection.LocalAudioTracks"/>
            - <see cref="P:Microsoft.MixedReality.WebRTC.PeerConnection.LocalVideoTracks"/>
            - <see cref="P:Microsoft.MixedReality.WebRTC.PeerConnection.RemoteAudioTracks"/>
            - <see cref="P:Microsoft.MixedReality.WebRTC.PeerConnection.RemoteVideoTracks"/>
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection._renegotiationNeededEvent">
            <summary>
            Implementation of <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.RenegotiationNeeded"/> adding the capability to delay the event.
            This allows <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddTransceiver(Microsoft.MixedReality.WebRTC.MediaKind,Microsoft.MixedReality.WebRTC.TransceiverInitSettings)"/> to wait until the
            newly created transceiver wrapper is fully instantiated to dispatch the event.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.#ctor">
            <summary>
            Create a new peer connection object. The object is initially created empty, and cannot be used
            until <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.InitializeAsync(Microsoft.MixedReality.WebRTC.PeerConnectionConfiguration,System.Threading.CancellationToken)"/> has completed
            successfully.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.InitializeAsync(Microsoft.MixedReality.WebRTC.PeerConnectionConfiguration,System.Threading.CancellationToken)">
             <summary>
             Initialize the current peer connection object asynchronously.
            
             Most other methods will fail unless this call completes successfully, as it initializes the
             underlying native implementation object required to create and manipulate the peer connection.
            
             Once this call asynchronously completed, the <see cref="P:Microsoft.MixedReality.WebRTC.PeerConnection.Initialized"/> property becomes <c>true</c>.
             </summary>
             <param name="config">Configuration for initializing the peer connection.</param>
             <param name="token">Optional cancellation token for the initialize task. This is only used if
             the singleton task was created by this call, and not a prior call.</param>
             <returns>The singleton task used to initialize the underlying native peer connection.</returns>
             <remarks>This method is multi-thread safe, and will always return the same task object
             from the first call to it until the peer connection object is deinitialized. This allows
             multiple callers to all execute some action following the initialization, without the need
             to force a single caller and to synchronize with it.</remarks>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.Close">
            <summary>
            Close the peer connection and destroy the underlying native resources.
            </summary>
            <remarks>This is equivalent to <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.Dispose"/>.</remarks>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.Dispose"/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.Dispose">
            <summary>
            Dispose of native resources by closing the peer connection.
            </summary>
            <remarks>This is equivalent to <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.Close"/>.</remarks>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.Close"/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddTransceiver(Microsoft.MixedReality.WebRTC.MediaKind,Microsoft.MixedReality.WebRTC.TransceiverInitSettings)">
             <summary>
             Add to the current connection a new media transceiver.
            
             A transceiver is a container for a pair of media tracks, one local sending to the remote
             peer, and one remote receiving from the remote peer. Both are optional, and the transceiver
             can be in receive-only mode (no local track), in send-only mode (no remote track), or
             inactive (neither local nor remote track).
            
             Once a transceiver is added to the peer connection, it cannot be removed, but its tracks can be
             changed (this requires some renegotiation).
             </summary>
             <param name="mediaKind">Kind of media the transeiver is transporting.</param>
             <param name="settings">Settings to initialize the new transceiver.</param>
             <returns>The newly created transceiver.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddDataChannelAsync(System.UInt16,System.String,System.Boolean,System.Boolean,System.Threading.CancellationToken)">
             <summary>
             Add a new out-of-band data channel with the given ID.
            
             A data channel is branded out-of-band when the peers agree on an identifier by any mean
             not known to WebRTC, and both open a data channel with that ID. The WebRTC will match the
             incoming and outgoing pipes by this ID to allow sending and receiving through that channel.
            
             This requires some external mechanism to agree on an available identifier not otherwise taken
             by another channel, and also requires to ensure that both peers explicitly open that channel.
             The advantage of in-band data channels is that no SDP session renegotiation is needed, except
             for the very first data channel added (in-band or out-of-band) which requires a negotiation
             for the SCTP handshake (see remarks).
             </summary>
             <param name="id">The unique data channel identifier to use.</param>
             <param name="label">The data channel name.</param>
             <param name="ordered">Indicates whether data channel messages are ordered (see
             <see cref="P:Microsoft.MixedReality.WebRTC.DataChannel.Ordered"/>).</param>
             <param name="reliable">Indicates whether data channel messages are reliably delivered
             (see <see cref="P:Microsoft.MixedReality.WebRTC.DataChannel.Reliable"/>).</param>
             <param name="cancellationToken">Cancellation token for the task returned.</param>
             <returns>Returns a task which completes once the data channel is created.</returns>
             <exception xref="InvalidOperationException">The peer connection is not initialized.</exception>
             <exception cref="T:Microsoft.MixedReality.WebRTC.SctpNotNegotiatedException">SCTP not negotiated. Call <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.CreateOffer"/> first.</exception>
             <exception xref="ArgumentOutOfRangeException">Invalid data channel ID, must be in [0:65535].</exception>
             <remarks>
             Data channels use DTLS over SCTP, which ensure in particular that messages are encrypted. To that end,
             while establishing a connection with the remote peer, some specific SCTP handshake must occur. This
             handshake is only performed if at least one data channel was added to the peer connection when the
             connection starts its negotiation with <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.CreateOffer"/>. Therefore, if the user wants to use
             a data channel at any point during the lifetime of this peer connection, it is critical to add at least
             one data channel before <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.CreateOffer"/> is called. Otherwise all calls will fail with an
             <see cref="T:Microsoft.MixedReality.WebRTC.SctpNotNegotiatedException"/> exception.
             </remarks>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddDataChannelAsync(System.String,System.Boolean,System.Boolean,System.Threading.CancellationToken)">
             <summary>
             Add a new in-band data channel whose ID will be determined by the implementation.
            
             A data channel is branded in-band when one peer requests its creation to the WebRTC core,
             and the implementation negotiates with the remote peer an appropriate ID by sending some
             SDP offer message. In that case once accepted the other peer will automatically create the
             appropriate data channel on its side with that same ID, and the ID will be returned on
             both sides to the user for information.
            
             Compared to out-of-band messages, this requires exchanging some SDP messages, but avoids having
             to agree on a common unused ID and having to explicitly open the data channel on both sides.
             </summary>
             <param name="label">The data channel name.</param>
             <param name="ordered">Indicates whether data channel messages are ordered (see
             <see cref="P:Microsoft.MixedReality.WebRTC.DataChannel.Ordered"/>).</param>
             <param name="reliable">Indicates whether data channel messages are reliably delivered
             (see <see cref="P:Microsoft.MixedReality.WebRTC.DataChannel.Reliable"/>).</param>
             <param name="cancellationToken">Cancellation token for the task returned.</param>
             <returns>Returns a task which completes once the data channel is created.</returns>
             <exception xref="System.InvalidOperationException">The peer connection is not initialized.</exception>
             <exception cref="T:Microsoft.MixedReality.WebRTC.SctpNotNegotiatedException">SCTP not negotiated. Call <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.CreateOffer"/> first.</exception>
             <exception xref="System.ArgumentOutOfRangeException">Invalid data channel ID, must be in [0:65535].</exception>
             <remarks>
             See the critical remark about SCTP handshake in <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddDataChannelAsync(System.UInt16,System.String,System.Boolean,System.Boolean,System.Threading.CancellationToken)"/>.
             </remarks>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddDataChannelAsyncImpl(System.Int32,System.String,System.Boolean,System.Boolean,System.Threading.CancellationToken)">
            <summary>
            Add a new in-band or out-of-band data channel.
            </summary>
            <param name="id">Identifier in [0:65535] of the out-of-band data channel, or <c>-1</c> for in-band.</param>
            <param name="label">The data channel name.</param>
            <param name="ordered">Indicates whether data channel messages are ordered (see
            <see cref="P:Microsoft.MixedReality.WebRTC.DataChannel.Ordered"/>).</param>
            <param name="reliable">Indicates whether data channel messages are reliably delivered
            (see <see cref="P:Microsoft.MixedReality.WebRTC.DataChannel.Reliable"/>).</param>
            <param name="cancellationToken">Cancellation token for the task returned.</param>
            <returns>Returns a task which completes once the data channel is created.</returns>
            <exception xref="System.InvalidOperationException">The peer connection is not initialized.</exception>
            <exception xref="System.InvalidOperationException">SCTP not negotiated.</exception>
            <exception xref="System.ArgumentOutOfRangeException">Invalid data channel ID, must be in [0:65535].</exception>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.RemoveDataChannel(Microsoft.MixedReality.WebRTC.DataChannel)">
            <summary>
            Remove an existing data channel from the peer connection and destroy its native implementation.
            </summary>
            <param name="dataChannel">The data channel to remove and destroy.</param>
            <exception xref="System.ArgumentException">The data channel is not owned by this peer connection.</exception>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddIceCandidate(Microsoft.MixedReality.WebRTC.IceCandidate)">
            <summary>
            Inform the WebRTC peer connection of a newly received ICE candidate.
            </summary>
            <param name="candidate">The ICE candidate received from the remote peer.</param>
            <exception xref="InvalidOperationException">The peer connection is not initialized.</exception>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.CreateOffer">
            <summary>
            Create an SDP offer message as an attempt to establish a connection.
            Once the message is ready to be sent, the <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.LocalSdpReadytoSend"/> event is fired
            to allow the user to send that message to the remote peer via its selected signaling solution.
            </summary>
            <returns><c>true</c> if the offer creation task was successfully submitted.</returns>
            <exception xref="InvalidOperationException">The peer connection is not initialized.</exception>
            <remarks>
            The SDP offer message is not successfully created until the <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.LocalSdpReadytoSend"/>
            event is triggered, and may still fail even if this method returns <c>true</c>, for example if
            the peer connection is not in a valid state to create an offer.
            </remarks>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.CreateAnswer">
            <summary>
            Create an SDP answer message to a previously-received offer, to accept a connection.
            Once the message is ready to be sent, the <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.LocalSdpReadytoSend"/> event is fired
            to allow the user to send that message to the remote peer via its selected signaling solution.
            Note that this cannot be called before <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.SetRemoteDescriptionAsync(Microsoft.MixedReality.WebRTC.SdpMessage)"/>
            successfully completed and applied the remote offer.
            </summary>
            <returns><c>true</c> if the answer creation task was successfully submitted.</returns>
            <exception xref="InvalidOperationException">The peer connection is not initialized.</exception>
            <remarks>
            The SDP answer message is not successfully created until the <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.LocalSdpReadytoSend"/>
            event is triggered, and may still fail even if this method returns <c>true</c>, for example if
            the peer connection is not in a valid state to create an answer.
            </remarks>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.SetBitrate(System.Nullable{System.UInt32},System.Nullable{System.UInt32},System.Nullable{System.UInt32})">
            <summary>
            Set the bitrate allocated to all RTP streams sent by this connection.
            Other limitations might affect these limits and are respected (for example "b=AS" in SDP).
            </summary>
            <param name="minBitrateBps">Minimum bitrate in bits per second.</param>
            <param name="startBitrateBps">Start/current target bitrate in bits per second.</param>
            <param name="maxBitrateBps">Maximum bitrate in bits per second.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.SetRemoteDescriptionAsync(Microsoft.MixedReality.WebRTC.SdpMessage)">
             <summary>
             Pass the given SDP description received from the remote peer via signaling to the
             underlying WebRTC implementation, which will parse and use it.
            
             This must be called by the signaler when receiving a message. Once this operation
             has completed, it is safe to call <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.CreateAnswer"/>.
             </summary>
             <param name="message">The SDP message</param>
             <returns>Returns a task which completes once the remote description has been applied and transceivers
             have been updated.</returns>
             <exception xref="InvalidOperationException">The peer connection is not initialized, or the peer connection
             is not in an expected state to apply the given message.</exception>
             <exception xref="ArgumentException">At least one of the arguments is invalid, including a malformed SDP
             message that failed to be parsed.</exception>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelStats">
            <summary>
            Subset of RTCDataChannelStats. See <see href="https://www.w3.org/TR/webrtc-stats/#dcstats-dict*"/>
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelStats.TimestampUs">
            <summary>
            Unix timestamp (time since Epoch) of the statistics. For remote statistics, this is
            the time at which the information reached the local endpoint.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelStats.DataChannelIdentifier">
            <summary>
            <see cref="P:Microsoft.MixedReality.WebRTC.DataChannel.ID"/> of the data channel associated with these statistics.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelStats.MessagesSent">
            <summary>
            Total number of API message event sent.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelStats.BytesSent">
            <summary>
            Total number of payload bytes sent, excluding headers and paddings.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelStats.MessagesReceived">
            <summary>
            Total number of API message events received.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelStats.BytesReceived">
            <summary>
            Total number of payload bytes received, excluding headers and paddings.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.AudioSenderStats">
            <summary>
            Subset of RTCMediaStreamTrack (audio sender) and RTCOutboundRTPStreamStats.
            See <see href="https://www.w3.org/TR/webrtc-stats/#raststats-dict*"/>
            and <see href="https://www.w3.org/TR/webrtc-stats/#sentrtpstats-dict*"/>.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioSenderStats.TrackStatsTimestampUs">
            <summary>
            Unix timestamp (time since Epoch) of the audio statistics. For remote statistics,
            this is the time at which the information reached the local endpoint.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioSenderStats.TrackIdentifier">
            <summary>
            Track identifier.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioSenderStats.AudioLevel">
            <summary>
            Linear audio level of the media source, in [0:1] range, averaged over a small interval.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioSenderStats.TotalAudioEnergy">
            <summary>
            Total audio energy of the media source. For multi-channel sources (stereo, etc.) this is
            the highest energy of any of the channels for each sample.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioSenderStats.TotalSamplesDuration">
            <summary>
            Total duration in seconds of all the samples produced by the media source for the lifetime
            of the underlying internal statistics object. Like <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioSenderStats.TotalAudioEnergy"/> this is not
            affected by the number of channels per sample.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioSenderStats.RtpStatsTimestampUs">
            <summary>
            Unix timestamp (time since Epoch) of the RTP statistics. For remote statistics,
            this is the time at which the information reached the local endpoint.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioSenderStats.PacketsSent">
            <summary>
            Total number of RTP packets sent for this SSRC.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioSenderStats.BytesSent">
            <summary>
            Total number of bytes sent for this SSRC.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats">
            <summary>
            Subset of RTCMediaStreamTrack (audio receiver) and RTCInboundRTPStreamStats.
            See <see href="https://www.w3.org/TR/webrtc-stats/#aststats-dict*"/>
            and <see href="https://www.w3.org/TR/webrtc-stats/#inboundrtpstats-dict*"/>.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats.TrackStatsTimestampUs">
            <summary>
            Unix timestamp (time since Epoch) of the statistics. For remote statistics, this is
            the time at which the information reached the local endpoint.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats.TrackIdentifier">
            <summary>
            Track identifier.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats.AudioLevel">
            <summary>
            Linear audio level of the receiving track, in [0:1] range, averaged over a small interval.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats.TotalAudioEnergy">
            <summary>
            Total audio energy of the received track. For multi-channel sources (stereo, etc.) this is
            the highest energy of any of the channels for each sample.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats.TotalSamplesReceived">
            <summary>
            Total number of RTP samples received for this audio stream.
            Like <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats.TotalAudioEnergy"/> this is not affected by the number of channels per sample.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats.TotalSamplesDuration">
            <summary>
            Total duration in seconds of all the samples received (and thus counted by <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats.TotalSamplesReceived"/>).
            Like <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats.TotalAudioEnergy"/> this is not affected by the number of channels per sample.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats.RtpStatsTimestampUs">
            <summary>
            Unix timestamp (time since Epoch) of the RTP statistics. For remote statistics,
            this is the time at which the information reached the local endpoint.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats.PacketsReceived">
            <summary>
            Total number of RTP packets received for this SSRC.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats.BytesReceived">
            <summary>
            Total number of bytes received for this SSRC.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.VideoSenderStats">
            <summary>
            Subset of RTCMediaStreamTrack (video sender) and RTCOutboundRTPStreamStats.
            See <see href="https://www.w3.org/TR/webrtc-stats/#vsstats-dict*"/>
            and <see href="https://www.w3.org/TR/webrtc-stats/#sentrtpstats-dict*"/>.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoSenderStats.TrackStatsTimestampUs">
            <summary>
            Unix timestamp (time since Epoch) of the track statistics. For remote statistics,
            this is the time at which the information reached the local endpoint.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoSenderStats.TrackIdentifier">
            <summary>
            Track identifier.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoSenderStats.FramesSent">
            <summary>
            Total number of frames sent on this RTP stream.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoSenderStats.HugeFramesSent">
            <summary>
            Total number of huge frames sent by this RTP stream. Huge frames are frames that have
            an encoded size at least 2.5 times the average size of the frames.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoSenderStats.RtpStatsTimestampUs">
            <summary>
            Unix timestamp (time since Epoch) of the RTP statistics. For remote statistics,
            this is the time at which the information reached the local endpoint.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoSenderStats.PacketsSent">
            <summary>
            Total number of RTP packets sent for this SSRC.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoSenderStats.BytesSent">
            <summary>
            Total number of bytes sent for this SSRC.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoSenderStats.FramesEncoded">
            <summary>
            Total number of frames successfully encoded for this RTP media stream.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.VideoReceiverStats">
            <summary>
            Subset of RTCMediaStreamTrack (video receiver) + RTCInboundRTPStreamStats.
            See <see href="https://www.w3.org/TR/webrtc-stats/#rvststats-dict*"/>
            and <see href="https://www.w3.org/TR/webrtc-stats/#inboundrtpstats-dict*"/>
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoReceiverStats.TrackStatsTimestampUs">
            <summary>
            Unix timestamp (time since Epoch) of the track statistics. For remote statistics,
            this is the time at which the information reached the local endpoint.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoReceiverStats.TrackIdentifier">
            <summary>
            Track identifier.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoReceiverStats.FramesReceived">
            <summary>
            Total number of complete frames received on this RTP stream.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoReceiverStats.FramesDropped">
            <summary>
            Total number since the receiver was created of frames dropped prior to decode or
            dropped because the frame missed its display deadline for this receiver's track.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoReceiverStats.RtpStatsTimestampUs">
            <summary>
            Unix timestamp (time since Epoch) of the RTP statistics. For remote statistics,
            this is the time at which the information reached the local endpoint.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoReceiverStats.PacketsReceived">
            <summary>
            Total number of RTP packets received for this SSRC.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoReceiverStats.BytesReceived">
            <summary>
            Total number of bytes received for this SSRC.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.VideoReceiverStats.FramesDecoded">
            <summary>
            Total number of frames correctly decoded for this RTP stream, that would be displayed
            if no frames are dropped.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.TransportStats">
            <summary>
            Subset of RTCTransportStats.
            See <see href="https://www.w3.org/TR/webrtc-stats/#transportstats-dict*"/>.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.TransportStats.TimestampUs">
            <summary>
            Unix timestamp (time since Epoch) of the statistics. For remote statistics, this is
            the time at which the information reached the local endpoint.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.TransportStats.BytesSent">
            <summary>
            Total number of payload bytes sent on this <see cref="T:Microsoft.MixedReality.WebRTC.PeerConnection"/>, excluding
            headers and paddings.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.TransportStats.BytesReceived">
            <summary>
            Total number of payload bytes received on this <see cref="T:Microsoft.MixedReality.WebRTC.PeerConnection"/>, excluding
            headers and paddings.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.StatsReport">
            <summary>
            Snapshot of the statistics relative to a peer connection/track.
            The various stats objects can be read through <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.StatsReport.GetStats``1"/>.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.StatsReport.GetStats``1">
            <summary>
            Get all the instances of a specific stats type in the report.
            </summary>
            <typeparam name="T">
            Must be one of <see cref="T:Microsoft.MixedReality.WebRTC.PeerConnection.DataChannelStats"/>, <see cref="T:Microsoft.MixedReality.WebRTC.PeerConnection.AudioSenderStats"/>,
            <see cref="T:Microsoft.MixedReality.WebRTC.PeerConnection.AudioReceiverStats"/>, <see cref="T:Microsoft.MixedReality.WebRTC.PeerConnection.VideoSenderStats"/>, <see cref="T:Microsoft.MixedReality.WebRTC.PeerConnection.VideoReceiverStats"/>,
            <see cref="T:Microsoft.MixedReality.WebRTC.PeerConnection.TransportStats"/>.
            </typeparam>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.StatsReport.Dispose">
            <summary>
            Dispose of the report.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.GetSimpleStatsAsync">
            <summary>
            Get a snapshot of the statistics relative to the peer connection.
            </summary>
            <exception xref="InvalidOperationException">The peer connection is not initialized.</exception>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.ThrowIfConnectionNotOpen">
            <summary>
            Utility to throw an exception if a method is called before the underlying
            native peer connection has been initialized.
            </summary>
            <exception xref="InvalidOperationException">The peer connection is not initialized.</exception>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.PeerConnection.FrameHeightRoundMode">
            <summary>
            Frame height round mode.
            </summary>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.SetFrameHeightRoundMode(Microsoft.MixedReality.WebRTC.PeerConnection.FrameHeightRoundMode)"/>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.FrameHeightRoundMode.None">
            <summary>
            Leave frames unchanged.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.FrameHeightRoundMode.Crop">
            <summary>
            Crop frame height to the nearest multiple of 16.
            ((height - nearestLowerMultipleOf16) / 2) rows are cropped from the top and
            (height - nearestLowerMultipleOf16 - croppedRowsTop) rows are cropped from the bottom.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.PeerConnection.FrameHeightRoundMode.Pad">
            <summary>
            Pad frame height to the nearest multiple of 16.
            ((nearestHigherMultipleOf16 - height) / 2) rows are added symmetrically at the top and
            (nearestHigherMultipleOf16 - height - addedRowsTop) rows are added symmetrically at the bottom.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.SetFrameHeightRoundMode(Microsoft.MixedReality.WebRTC.PeerConnection.FrameHeightRoundMode)">
             <summary>
             [HoloLens 1 only]
             Use this function to select whether resolutions where height is not multiple of 16 pixels
             should be cropped, padded, or left unchanged.
            
             Default is <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection.FrameHeightRoundMode.Crop"/> to avoid severe artifacts produced by
             the H.264 hardware encoder on HoloLens 1 due to a bug with the encoder. This is the
             recommended value, and should be used unless cropping discards valuable data in the top and
             bottom rows for a given usage, in which case <see cref="F:Microsoft.MixedReality.WebRTC.PeerConnection.FrameHeightRoundMode.Pad"/> can
             be used as a replacement but may still produce some mild artifacts.
            
             This has no effect on other platforms.
             </summary>
             <param name="value">The rounding mode for video frames.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.OnLocalSdpReadytoSend(Microsoft.MixedReality.WebRTC.SdpMessageType,System.String)">
            <summary>
            Callback invoked by the internal WebRTC implementation when it needs a SDP message
            to be dispatched to the remote peer.
            </summary>
            <param name="type">The SDP message type.</param>
            <param name="sdp">The SDP message content.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.OnTransceiverAdded(Microsoft.MixedReality.WebRTC.Transceiver)">
            <summary>
            Callback on transceiver created for the peer connection, irrelevant of whether
            it has tracks or not. This is called both when created from the managed side or
            from the native side.
            </summary>
            <param name="tr">The newly created transceiver which has this peer connection as owner</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.OnAudioTrackAdded(Microsoft.MixedReality.WebRTC.RemoteAudioTrack,Microsoft.MixedReality.WebRTC.Transceiver)">
            <summary>
            Callback invoked by the native implementation when a transceiver starts receiving,
            and a remote audio track is created as a result to receive its audio data.
            </summary>
            <param name="track">The newly created remote audio track.</param>
            <param name="transceiver">The audio transceiver now receiving from the remote peer.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.OnAudioTrackRemoved(Microsoft.MixedReality.WebRTC.RemoteAudioTrack)">
            <summary>
            Callback invoked by the native implementation when a transceiver stops receiving,
            and a remote audio track is removed from it as a result.
            </summary>
            <param name="track">The remote audio track removed from the audio transceiver.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.OnVideoTrackAdded(Microsoft.MixedReality.WebRTC.RemoteVideoTrack,Microsoft.MixedReality.WebRTC.Transceiver)">
            <summary>
            Callback invoked by the native implementation when a transceiver starts receiving,
            and a remote video track is created as a result to receive its video data.
            </summary>
            <param name="track">The newly created remote video track.</param>
            <param name="transceiver">The video transceiver now receiving from the remote peer.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.OnVideoTrackRemoved(Microsoft.MixedReality.WebRTC.RemoteVideoTrack)">
            <summary>
            Callback invoked by the native implementation when a transceiver stops receiving,
            and a remote video track is removed from it as a result.
            </summary>
            <param name="track">The remote video track removed from the video transceiver.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.PeerConnection.ToString">
            <inheritdoc/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.RemoteAudioTrack">
             <summary>
             Audio track receiving audio frames from the remote peer.
             </summary>
             <remarks>
             Instances of this class are created by <see cref="T:Microsoft.MixedReality.WebRTC.PeerConnection"/> when a negotiation
             adds tracks sent by the remote peer.
            
             New tracks are automatically played on the system audio device after
             <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.AudioTrackAdded"/> is fired on track creation. To avoid the track
             being played, call <see cref="M:Microsoft.MixedReality.WebRTC.RemoteAudioTrack.OutputToDevice(System.Boolean)"/> in a <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.AudioTrackAdded"/>
             handler (or later).
             </remarks>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.RemoteAudioTrack.Enabled">
            <summary>
            Enabled status of the track. If enabled, receives audio frames from the remote peer as
            expected. If disabled, does not receive anything (silence).
            </summary>
            <remarks>
            Reading the value of this property after the track has been disposed is valid, and returns
            <c>false</c>.
            The remote audio track enabled status is controlled by the remote peer only.
            </remarks>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.RemoteAudioTrack.AudioFrameReady">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.RemoteAudioTrack.OutputToDevice(System.Boolean)">
             <summary>
             Output the audio track to the WebRTC audio device.
             </summary>
             <remarks>
             The default behavior is for every remote audio frame to be passed to
             remote audio frame callbacks, as well as output automatically to the
             audio device used by WebRTC. If |false| is passed to this function, remote
             audio frames will still be received and passed to callbacks, but won't be
             output to the audio device.
            
             NOTE: Changing the default behavior is not supported on UWP.
             </remarks>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.RemoteAudioTrack.IsOutputToDevice">
            <summary>
            Returns whether the track is output directly to the system audio device.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.RemoteAudioTrack.CreateReadBuffer">
            <inheritdoc/>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.RemoteAudioTrack._nativeHandle">
            <summary>
            Handle to the native RemoteAudioTrack object.
            </summary>
            <remarks>
            In native land this is a <code>mrsRemoteAudioTrackHandle</code>.
            </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.RemoteAudioTrack._selfHandle">
            <summary>
            Handle to self for interop callbacks. This adds a reference to the current object, preventing
            it from being garbage-collected.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.RemoteAudioTrack._interopCallbackArgs">
            <summary>
            Callback arguments to ensure delegates registered with the native layer don't go out of scope.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.RemoteAudioTrack.DestroyNative">
            <summary>
            Dispose of the native track. Invoked by its owner (<see cref="T:Microsoft.MixedReality.WebRTC.PeerConnection"/>).
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.RemoteAudioTrack.ToString">
            <inheritdoc/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.RemoteVideoTrack">
            <summary>
            Video track receiving video frames from the remote peer.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.RemoteVideoTrack.Enabled">
            <summary>
            Enabled status of the track. If enabled, receives video frames from the remote peer as
            expected. If disabled, receives only black frames instead.
            </summary>
            <remarks>
            Reading the value of this property after the track has been disposed is valid, and returns
            <c>false</c>.
            The remote video track enabled status is controlled by the remote peer only.
            </remarks>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.RemoteVideoTrack.FrameEncoding">
            <inheritdoc/>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.RemoteVideoTrack.I420AVideoFrameReady">
            <inheritdoc/>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.RemoteVideoTrack.Argb32VideoFrameReady">
            <inheritdoc/>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.RemoteVideoTrack._nativeHandle">
            <summary>
            Handle to the native RemoteVideoTrack object.
            </summary>
            <remarks>
            In native land this is a <code>Microsoft::MixedReality::WebRTC::RemoteVideoTrackHandle</code>.
            </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.RemoteVideoTrack._selfHandle">
            <summary>
            Handle to self for interop callbacks. This adds a reference to the current object, preventing
            it from being garbage-collected.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.RemoteVideoTrack.DestroyNative">
            <summary>
            Dispose of the native track. Invoked by its owner (<see cref="T:Microsoft.MixedReality.WebRTC.PeerConnection"/>).
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.RemoteVideoTrack.ToString">
            <inheritdoc/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.TaskExtensions">
            <summary>
            Collection of extension methods for <xref href="System.Threading.Tasks.Task"/>.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.TaskExtensions.IgnoreCancellation(System.Threading.Tasks.Task)">
            <summary>
            Prevents <see xref="TaskCanceledException"/> or <see xref="OperationCanceledException"/> from trickling up.
            </summary>
            <param name="task">The task to ignore exceptions for.</param>
            <returns>A wrapping task for the given task.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.TaskExtensions.IgnoreCancellation``1(System.Threading.Tasks.Task{``0},``0)">
            <summary>
            Prevents <see xref="TaskCanceledException"/> or <see xref="OperationCanceledException"/> from trickling up.
            </summary>
            <typeparam name="T">The result type of the Task.</typeparam>
            <param name="task">The task to ignore exceptions for.</param>
            <param name="defaultCancellationReturn">The default value to return in case the task is cancelled.</param>
            <returns>A wrapping task for the given task.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.TaskExtensions.AsTask(System.Threading.CancellationToken)">
            <summary>
            A simple helper to enable "awaiting" a <see xref="CancellationToken"/> by creating a task wrapping it.
            </summary>
            <param name="cancellationToken">The <see xref="CancellationToken"/> to await.</param>
            <returns>The task that can be awaited.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.TaskExtensions.Unless(System.Threading.Tasks.Task,System.Threading.CancellationToken)">
            <summary>
            The task will be awaited until the cancellation token is triggered. (await task unless cancelled).
            </summary>
            <remarks>This is different from cancelling the task. The use case is to enable a calling method 
            bow out of the await that it can't cancel, but doesn't require completion/cancellation in order to cancel it's own execution.</remarks>
            <param name="task">The task to await.</param>
            <param name="cancellationToken">The cancellation token to stop awaiting.</param>
            <returns>The task that can be awaited unless the cancellation token is triggered.</returns>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Tracing.MainEventSource">
            <summary>
            Global event source for logging and tracing.
            
            Usage:
            - Select a unique event ID.
            - Define an event as a public method with a name related to the type of event to be logged, and
              tagged with the <see xref="System.Diagnostics.Tracing.EventAttribute"/> attribute, specifying
              the event ID as the first argument.
            - Use one of the WriteEvent methods to write the event and any associated field.
            - In code, use the <see cref="F:Microsoft.MixedReality.WebRTC.Tracing.MainEventSource.Log"/> static instance to log some event by calling
              the event public method just defined.
            </summary>
            <remarks>
            On Windows, this logs ETW events to the "Microsoft.MixedReality.WebRTC" event provider
            (GUID: 00AEE89E-B531-4F20-A2C5-D02F37CB6AA1) which can be captured with Windows Performance
            Recorder (WPR). The <c>tools/tracing/</c> folder contains a WPR profile (*.wprp) which can be
            imported in wpr.exe to activate that event provider and record its events.
            
            Some platforms like Unity do not support ETW events, and will transpile any <c>WriteEvent()</c>
            call to an exception when using IL2CPP. To handle that case, this class is defined as doing nothing
            unless the MR_SHARING_ENABLE_TRACING symbol is defined.
            </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Tracing.MainEventSource.Log">
            <summary>
            Global event source instance to use for logging events.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Tracing.MainEventSource.Keywords">
            <summary>
            Event categories.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Tracing.MainEventSource.Keywords.Connection">
            <summary>
            Event related to the peer connection.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Tracing.MainEventSource.Keywords.Sdp">
            <summary>
            Event related to the SDP session management, like sending SDP messages
            and ICE candidates, and handling renegotiations.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Tracing.MainEventSource.Keywords.Media">
            <summary>
            Event related to audio and video (tracks, sources, capture, ...).
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Tracing.MainEventSource.Keywords.DataChannel">
            <summary>
            Event related to data channels.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Tracing.MainEventSource.Initialize">
            <summary>
            Initialize the event source. This must be called once before any use of the event source.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.MediaKind">
            <summary>
            Type of media track or media transceiver.
            </summary>
            <remarks>
            This is the projection of <c>mrsMediaKind</c> from the interop API.
            </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.MediaKind.Audio">
            <summary>
            Audio data.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.MediaKind.Video">
            <summary>
            Video data.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.TransceiverAssociatedDelegate">
            <summary>
            Delegate for the <see cref="E:Microsoft.MixedReality.WebRTC.Transceiver.Associated"/> event.
            </summary>
            <param name="transceiver">The transceiver becoming associated.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.TransceiverDirectionChangedDelegate">
            <summary>
            Delegate for the <see cref="E:Microsoft.MixedReality.WebRTC.Transceiver.DirectionChanged"/> event.
            </summary>
            <param name="transceiver">
            The transceiver whose <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.NegotiatedDirection"/> property changed.
            </param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Transceiver">
             <summary>
             Transceiver of a peer connection.
            
             A transceiver is a media "pipe" connecting the local and remote peers, and used to transmit media
             data (audio or video) between the peers. The transceiver has a media flow direction indicating whether
             it is sending and/or receiving any media, or is inactive. When sending some media, the transceiver's
             local track is used as the source of that media. Conversely, when receiving some media, that media is
             delivered to the remote media track of the transceiver. As a convenience, the local track can be null
             if the local peer does not have anything to send. In that case some empty media is automatically sent
             instead (black frames for video, silence for audio) at very reduced rate. To completely stop sending,
             the media direction must be changed instead.
            
             Transceivers are owned by the peer connection which creates them, and cannot be destroyed nor removed
             from the peer connection. They become invalid when the peer connection is closed, and should not be
             used after that.
             </summary>
             <remarks>
             This object corresponds roughly to the same-named notion in the WebRTC 1.0 standard when using the
             Unified Plan SDP semantic.
            
             For Plan B semantic, where RTP transceivers are not available, this wrapper tries to emulate the
             transceiver concept of the Unified Plan semantic, and is therefore providing an abstraction over the
             WebRTC concept of transceivers.
             </remarks>
             <seealso cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddTransceiver(Microsoft.MixedReality.WebRTC.MediaKind,Microsoft.MixedReality.WebRTC.TransceiverInitSettings)"/>
             <seealso cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.Close"/>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Transceiver.Direction">
            <summary>
            Direction of the media flowing inside the transceiver.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Transceiver.Direction.SendReceive">
            <summary>
            Transceiver is both sending to and receiving from the remote peer connection.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Transceiver.Direction.SendOnly">
            <summary>
            Transceiver is sending to the remote peer, but is not receiving any media from the remote peer.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Transceiver.Direction.ReceiveOnly">
            <summary>
            Transceiver is receiving from the remote peer, but is not sending any media to the remote peer.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Transceiver.Direction.Inactive">
            <summary>
            Transceiver is inactive, neither sending nor receiving any media data.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Transceiver.Name">
            <summary>
            A name for the transceiver, used for logging and debugging only.
            This can be set on construction if the transceiver is created by the local peer using
            <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddTransceiver(Microsoft.MixedReality.WebRTC.MediaKind,Microsoft.MixedReality.WebRTC.TransceiverInitSettings)"/>, or will
            be generated by the implementation otherwise.
            There is no guarantee of unicity; this name is only informational.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Transceiver.MediaKind">
            <summary>
            Type of media carried by the transceiver, and by extension type of media of its tracks.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Transceiver.PeerConnection">
            <summary>
            Peer connection this transceiver is part of.
            </summary>
            <seealso cref="T:Microsoft.MixedReality.WebRTC.PeerConnection"/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Transceiver.MlineIndex">
            <summary>
            Index of the media line in the SDP protocol for this transceiver. If the transceiver is not
            yet associated with a media line, this index has a negative value (invalid). Transceivers are
            associated when an offer, local or remote, is applied to the local peer connection. Consequently,
            transceivers created as a result of applying a remote offer are created in an associated state,
            with a media line index already valid, while transceivers created locally by the peer connection
            have an invalid index until the next offer.
            </summary>
            <remarks>
            For Plan B semantic (<see cref="F:Microsoft.MixedReality.WebRTC.SdpSemantic.PlanB"/>), the media line index is not present
            in the SDP protocol. Instead it is simulated by the implementation, which attempts to emulate
            the behavior of the Unified Plan semantic over an actual Plan B protocol.
            </remarks>
            <seealso cref="E:Microsoft.MixedReality.WebRTC.Transceiver.Associated"/>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.Transceiver.Associated">
            <summary>
            Event raised when the transceiver is associated with a media line, which therefore makes
            the <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.MlineIndex"/> property take a valid positive value.
            </summary>
            <remarks>
            The event is not raised if the transceiver is created in an associated state, that is if
            the transceiver is already associated when <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.TransceiverAdded"/>
            is raised to signal it was added. This happens when the transceiver is created as part of
            applying a remote offer. In short, this event is raised only for transceivers created locally
            with <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.AddTransceiver(Microsoft.MixedReality.WebRTC.MediaKind,Microsoft.MixedReality.WebRTC.TransceiverInitSettings)"/>.
            </remarks>
            <seealso cref="P:Microsoft.MixedReality.WebRTC.Transceiver.MlineIndex"/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Transceiver.DesiredDirection">
             <summary>
             Transceiver direction desired by the user.
            
             Once changed by the user, this value is the next direction that will be negotiated when
             calling <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.CreateOffer"/> or <see cref="M:Microsoft.MixedReality.WebRTC.PeerConnection.CreateAnswer"/>.
            
             After the negotiation is completed, this is generally equal to <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.NegotiatedDirection"/>,
             unless the offer was partially rejected, for example if the local peer offered to send and receive
             some media but the remote peer only accepted to receive.
            
             Changing the value of this property triggers a <see cref="E:Microsoft.MixedReality.WebRTC.PeerConnection.RenegotiationNeeded"/> event.
             </summary>
             <seealso cref="P:Microsoft.MixedReality.WebRTC.Transceiver.NegotiatedDirection"/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Transceiver.NegotiatedDirection">
            <summary>
            Last negotiated transceiver direction. This is constant when changing <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.DesiredDirection"/>,
            and is only udpated after an SDP session negotiation. This might be different from the desired
            direction if for example the local peer asked to receive but the remote peer refused. This is the
            actual direction the media is effectively transported in at any point in time.
            </summary>
            <seealso cref="P:Microsoft.MixedReality.WebRTC.Transceiver.DesiredDirection"/>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.Transceiver.DirectionChanged">
            <summary>
            Event raised when the <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.NegotiatedDirection"/> changed, which occurs after applying
            a local or remote description. This is a convenience event raised only when the direction effectively
            changed, to avoid having to parse all transceivers for change after each description was applied.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Transceiver.StreamIDs">
            <summary>
            List of stream IDs associated with the transceiver.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Transceiver.LocalTrack">
            <summary>
            Local track attached to the transceiver, which is used to send data to the remote peer
            if <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.NegotiatedDirection"/> includes sending.
            This cannot be assigned directly; instead use <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.LocalAudioTrack"/> or
            <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.LocalVideoTrack"/> depending on the media kind of the transceiver.
            </summary>
            <seealso cref="P:Microsoft.MixedReality.WebRTC.Transceiver.LocalAudioTrack"/>
            <seealso cref="P:Microsoft.MixedReality.WebRTC.Transceiver.LocalVideoTrack"/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Transceiver.LocalAudioTrack">
             <summary>
             Local audio track attached to the transceiver, if <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.MediaKind"/> is
             <see cref="F:Microsoft.MixedReality.WebRTC.MediaKind.Audio"/>.
            
             The property has two uses:
             - as a convenience getter to retrieve <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.LocalTrack"/> already cast to a
               <see cref="T:Microsoft.MixedReality.WebRTC.LocalAudioTrack"/> type, or <c>null</c> if the transceiver
               media kind is <see cref="F:Microsoft.MixedReality.WebRTC.MediaKind.Video"/>.
             - to attach a new local audio track if the transceiver is an audio transceiver;
               otherwise this throws a <see cref="T:System.ArgumentException"/>.
             </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Transceiver.LocalVideoTrack">
             <summary>
             Local video track attached to the transceiver, if <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.MediaKind"/> is
             <see cref="F:Microsoft.MixedReality.WebRTC.MediaKind.Video"/>.
            
             The property has two uses:
             - as a convenience getter to retrieve <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.LocalTrack"/> already cast to a
               <see cref="T:Microsoft.MixedReality.WebRTC.LocalVideoTrack"/> type, or <c>null</c> if the transceiver
               media kind is <see cref="F:Microsoft.MixedReality.WebRTC.MediaKind.Audio"/>.
             - to attach a new local video track if the transceiver is a video transceiver;
               otherwise this throws a <see cref="T:System.ArgumentException"/>.
             </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Transceiver.RemoteTrack">
            <summary>
            Remote track attached to the transceiver, which is used to receive data from the
            remote peer if <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.NegotiatedDirection"/> includes receiving.
            This cannot be assigned. This is updated automatically when the remote track is
            created or destroyed as part of a renegotiation.
            </summary>
            <seealso cref="P:Microsoft.MixedReality.WebRTC.Transceiver.RemoteAudioTrack"/>
            <seealso cref="P:Microsoft.MixedReality.WebRTC.Transceiver.RemoteVideoTrack"/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Transceiver.RemoteAudioTrack">
            <summary>
            Remote audio track attached to the transceiver, if <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.MediaKind"/> is <see cref="F:Microsoft.MixedReality.WebRTC.MediaKind.Audio"/>.
            This is equivalent to <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.RemoteTrack"/> for audio transceivers, and <c>null</c> otherwise.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Transceiver.RemoteVideoTrack">
            <summary>
            Remote video track attached to the transceiver, if <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.MediaKind"/> is <see cref="F:Microsoft.MixedReality.WebRTC.MediaKind.Video"/>.
            This is equivalent to <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.RemoteTrack"/> for video transceivers, and <c>null</c> otherwise.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Transceiver._desiredDirection">
            <summary>
            Backing field for <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.DesiredDirection"/>.
            </summary>
            <seealso cref="P:Microsoft.MixedReality.WebRTC.Transceiver.DesiredDirection"/>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Transceiver._nativeHandle">
            <summary>
            Handle to the native transceiver object, valid until <see cref="M:Microsoft.MixedReality.WebRTC.Transceiver.CleanUpAfterNativeDestroyed"/>
            is called by the native implementation when the transceiver is destroyed as part
            of the peer connection closing.
            </summary>
            <remarks>
            In native land this is a <code>mrsTransceiverHandle</code>.
            </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Transceiver._argsRef">
            <summary>
            Reference to the struct keeping the callback delegates alive while registered with
            the native implementation.
            This should be released with <see cref="M:Microsoft.MixedReality.WebRTC.Interop.Utils.ReleaseWrapperRef(System.IntPtr)"/>.
            </summary>
            <seealso cref="M:Microsoft.MixedReality.WebRTC.Interop.TransceiverInterop.RegisterCallbacks(Microsoft.MixedReality.WebRTC.Transceiver,System.IntPtr@)"/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Transceiver.#ctor(Microsoft.MixedReality.WebRTC.Interop.TransceiverInterop.TransceiverHandle,Microsoft.MixedReality.WebRTC.MediaKind,Microsoft.MixedReality.WebRTC.PeerConnection,System.Int32,System.String,System.String[],Microsoft.MixedReality.WebRTC.Transceiver.Direction)">
            <summary>
            Create a new transceiver associated with a given peer connection.
            </summary>
            <param name="handle">Handle to the native transceiver object.</param>
            <param name="mediaKind">The media kind of the transceiver and its tracks.</param>
            <param name="peerConnection">The peer connection owning this transceiver.</param>
            <param name="mlineIndex">The transceiver media line index in SDP.</param>
            <param name="name">The transceiver name.</param>
            <param name="streamIDs">Collection of stream IDs the transceiver is associated with, as set by the peer which created it.</param>
            <param name="initialDesiredDirection">Initial value to initialize <see cref="P:Microsoft.MixedReality.WebRTC.Transceiver.DesiredDirection"/> with.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Transceiver.SetLocalTrackImpl(Microsoft.MixedReality.WebRTC.LocalMediaTrack)">
             <summary>
             Change the local audio track sending data to the remote peer.
            
             This detaches the previous local audio track if any, and attaches the new one instead.
             Note that the transceiver will only send some audio data to the remote peer if its
             negotiated direction includes sending some data and it has an attached local track to
             produce this data.
            
             This change is transparent to the session, and does not trigger any renegotiation.
             </summary>
             <param name="track">The new local audio track attached to the transceiver, and used to
             produce audio data to send to the remote peer if the transceiver is sending.
             Passing <c>null</c> is allowed, and will detach the current track if any.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Transceiver.CleanUpAfterNativeDestroyed">
             <summary>
             Callback invoked after the native transceiver has been destroyed, for clean-up.
             This is called by the peer connection when it closes, just before the C# transceiver
             object instance is destroyed.
            
             This replaces an hypothetical TransceiverRemoved callback, which doesn't exist to
             prevent confusion and underline the fact transceiver cannot be removed after being
             added to a peer connection, until that peer connection is closed and destroys them.
             </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Transceiver.OnAssociated(System.Int32)">
            <summary>
            Callback on internal implementation notifying that the transceiver was associated with
            a media line.
            </summary>
            <param name="mlineIndex">The index of the new media line the transceiver is associated with.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Transceiver.OnStateUpdated(System.Nullable{Microsoft.MixedReality.WebRTC.Transceiver.Direction},Microsoft.MixedReality.WebRTC.Transceiver.Direction)">
            <summary>
            Callback on internal implementation state changed to synchronize the cached state of this wrapper.
            </summary>
            <param name="negotiatedDirection">Current negotiated direction of the transceiver</param>
            <param name="desiredDirection">Current desired direction of the transceiver</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Transceiver.HasSend(Microsoft.MixedReality.WebRTC.Transceiver.Direction)">
            <summary>
            Check whether the given direction includes sending.
            </summary>
            <param name="dir">The direction to check.</param>
            <returns><c>true</c> if direction is <see cref="F:Microsoft.MixedReality.WebRTC.Transceiver.Direction.SendOnly"/> or <see cref="F:Microsoft.MixedReality.WebRTC.Transceiver.Direction.SendReceive"/>.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Transceiver.HasRecv(Microsoft.MixedReality.WebRTC.Transceiver.Direction)">
            <summary>
            Check whether the given direction includes receiving.
            </summary>
            <param name="dir">The direction to check.</param>
            <returns><c>true</c> if direction is <see cref="F:Microsoft.MixedReality.WebRTC.Transceiver.Direction.ReceiveOnly"/> or <see cref="F:Microsoft.MixedReality.WebRTC.Transceiver.Direction.SendReceive"/>.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Transceiver.HasSend(System.Nullable{Microsoft.MixedReality.WebRTC.Transceiver.Direction})">
            <summary>
            Check whether the given direction includes sending.
            </summary>
            <param name="dir">The direction to check.</param>
            <returns><c>true</c> if direction is <see cref="F:Microsoft.MixedReality.WebRTC.Transceiver.Direction.SendOnly"/> or <see cref="F:Microsoft.MixedReality.WebRTC.Transceiver.Direction.SendReceive"/>.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Transceiver.HasRecv(System.Nullable{Microsoft.MixedReality.WebRTC.Transceiver.Direction})">
            <summary>
            Check whether the given direction includes receiving.
            </summary>
            <param name="dir">The direction to check.</param>
            <returns><c>true</c> if direction is <see cref="F:Microsoft.MixedReality.WebRTC.Transceiver.Direction.ReceiveOnly"/> or <see cref="F:Microsoft.MixedReality.WebRTC.Transceiver.Direction.SendReceive"/>.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Transceiver.DirectionFromSendRecv(System.Boolean,System.Boolean)">
            <summary>
            Compute a transceiver direction from some send/receive booleans.
            </summary>
            <param name="hasSend">Does the direction includes sending?</param>
            <param name="hasRecv">Does the direction includes receiving?</param>
            <returns>The computed transceiver direction.</returns>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.I420AVideoFrame">
             <summary>
             Single video frame encoded in I420A format (triplanar YUV with optional alpha plane).
             See e.g. https://wiki.videolan.org/YUV/#I420 for details.
            
             The I420 format uses chroma downsampling in both directions, resulting in 12 bits per
             pixel. With the optional alpha plane, the size increases to 20 bits per pixel.
             </summary>
             <remarks>
             The use of <c>ref struct</c> is an optimization to avoid heap allocation on each frame while
             having a nicer-to-use container to pass a frame accross methods.
            
             The alpha plane is generically supported in this data structure, but actual support
             in the video tracks depend on the underlying implementation and the video codec used,
             and is generally not available.
             </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.I420AVideoFrame.width">
            <summary>
            Frame width, in pixels.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.I420AVideoFrame.height">
            <summary>
            Frame height, in pixels.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.I420AVideoFrame.dataY">
            <summary>
            Pointer to the Y plane buffer.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.I420AVideoFrame.dataU">
            <summary>
            Pointer to the U plane buffer.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.I420AVideoFrame.dataV">
            <summary>
            Pointer to the V plane buffer.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.I420AVideoFrame.dataA">
            <summary>
            Optional pointer to the alpha plane buffer, if any, or <c>null</c> if the frame has no alpha plane.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.I420AVideoFrame.strideY">
            <summary>
            Stride in bytes between rows of the Y plane.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.I420AVideoFrame.strideU">
            <summary>
            Stride in bytes between rows of the U plane.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.I420AVideoFrame.strideV">
            <summary>
            Stride in bytes between rows of the V plane.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.I420AVideoFrame.strideA">
            <summary>
            Stride in bytes between rows of the A plane, if present.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.I420AVideoFrame.CopyTo(System.Byte[])">
            <summary>
            Copy the frame content to a <xref href="System.Byte"/>[] buffer as a contiguous block of memory
            containing the Y, U, and V planes one after another, and the alpha plane at the end if present.
            </summary>
            <param name="buffer">The destination buffer to copy the frame to.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.I420AVideoFrameDelegate">
            <summary>
            Delegate used for events when an I420-encoded video frame has been produced
            and is ready for consumption.
            </summary>
            <param name="frame">The newly available I420-encoded video frame.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Argb32VideoFrame">
             <summary>
             Single video frame encoded in ARGB interleaved format (32 bits per pixel).
            
             The ARGB components are in the order of a little endian 32-bit integer, so
             0xAARRGGBB, or (B, G, R, A) as a sequence of bytes in memory with B first
             and A last.
             </summary>
             <remarks>
             The use of <c>ref struct</c> is an optimization to avoid heap allocation on each frame while
             having a nicer-to-use container to pass a frame accross methods.
             </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Argb32VideoFrame.width">
            <summary>
            Frame width, in pixels.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Argb32VideoFrame.height">
            <summary>
            Frame height, in pixels.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Argb32VideoFrame.data">
            <summary>
            Pointer to the data buffer containing the ARBG data for each pixel.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.Argb32VideoFrame.stride">
            <summary>
            Stride in bytes between the ARGB rows.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Argb32VideoFrameDelegate">
            <summary>
            Delegate used for events when an ARGB-encoded video frame has been produced
            and is ready for consumption.
            </summary>
            <param name="frame">The newly available ARGB-encoded video frame.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.IVideoFrameStorage">
            <summary>
            Interface for a storage of a single video frame.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.IVideoFrameStorage.Capacity">
            <summary>
            Storage capacity, in bytes.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.IVideoFrameStorage.Width">
            <summary>
            Frame width, in pixels.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.IVideoFrameStorage.Height">
            <summary>
            Frame height, in pixels.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.IVideoFrameStorage.Buffer">
            <summary>
            Raw storage buffer of capacity <see cref="P:Microsoft.MixedReality.WebRTC.IVideoFrameStorage.Capacity"/>.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.I420AVideoFrameStorage">
            <summary>
            Storage for a video frame encoded in I420+Alpha format.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.I420AVideoFrameStorage.Capacity">
            <summary>
            Total capacity of the storage, in bytes.
            This can be assigned to resize the storage.
            </summary>
            <remarks>
            Reading this property is equivalent to reading the <see xref="System.Array.LongLength"/>
            property of <see cref="P:Microsoft.MixedReality.WebRTC.I420AVideoFrameStorage.Buffer"/>.
            </remarks>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.I420AVideoFrameStorage.Width">
            <summary>
            Frame width, in pixels.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.I420AVideoFrameStorage.Height">
            <summary>
            Frame height, in pixels.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.I420AVideoFrameStorage.Buffer">
            <summary>
            Raw byte buffer containing the frame data.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.I420AVideoFrameStorage.Resize(System.UInt64)">
            <summary>
            Resize the internal buffer to the given capacity.
            This has no effect if the new capacity is smaller than the current one.
            </summary>
            <param name="capacity">The new desired capacity, in bytes.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.Argb32VideoFrameStorage">
            <summary>
            Storage for a video frame encoded in ARGB format.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Argb32VideoFrameStorage.Capacity">
            <summary>
            Total capacity of the storage, in bytes.
            This can be assigned to resize the storage.
            </summary>
            <remarks>
            Reading this property is equivalent to reading the <see xref="System.Array.LongLength"/>
            property of <see cref="P:Microsoft.MixedReality.WebRTC.Argb32VideoFrameStorage.Buffer"/>.
            </remarks>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Argb32VideoFrameStorage.Width">
            <summary>
            Frame width, in pixels.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Argb32VideoFrameStorage.Height">
            <summary>
            Frame height, in pixels.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.Argb32VideoFrameStorage.Buffer">
            <summary>
            Raw byte buffer containing the frame data.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.Argb32VideoFrameStorage.Resize(System.UInt64)">
            <summary>
            Resize the internal buffer to the given capacity.
            This has no effect if the new capacity is smaller than the current one.
            </summary>
            <param name="capacity">The new desired capacity, in bytes.</param>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.IVideoFrameQueue">
            <summary>
            Interface for a queue of video frames.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.IVideoFrameQueue.QueuedFramesPerSecond">
            <summary>
            Get the number of frames enqueued per seconds.
            This is generally an average statistics representing how fast a video source
            produces some video frames.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.IVideoFrameQueue.DequeuedFramesPerSecond">
            <summary>
            Get the number of frames enqueued per seconds.
            This is generally an average statistics representing how fast a video sink
            consumes some video frames, typically to render them.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.IVideoFrameQueue.DroppedFramesPerSecond">
            <summary>
            Get the number of frames dropped per seconds.
            This is generally an average statistics representing how many frames were
            enqueued by a video source but not dequeued fast enough by a video sink,
            meaning the video sink renders at a slower framerate than the source can produce.
            </summary>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1">
            <summary>
            Small queue of video frames received from a source and pending delivery to a sink.
            Used as temporary buffer between the WebRTC callback (push model) and the video
            player rendering (pull model). This also handles dropping frames when the source
            is faster than the sink, by limiting the maximum queue length.
            </summary>
            <typeparam name="T">The type of video frame storage</typeparam>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1.QueuedFramesPerSecond">
            <inheritdoc/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1.DequeuedFramesPerSecond">
            <inheritdoc/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1.DroppedFramesPerSecond">
            <inheritdoc/>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1._frameQueue">
            <summary>
            Queue of frames pending delivery to sink.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1._unusedFramePool">
            <summary>
            Pool of unused frames available for reuse, to avoid memory allocations.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1._maxQueueLength">
            <summary>
            Maximum queue length in number of frames.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1._traceId">
            <summary>
            Unique identifier representing the current object instance, to discriminate ETW events.
            </summary>
            <remarks>
            Ideally this would be based on ActivityID and ProcessID, but the former is not easy to use
            with the C# API, while also not fitting really well with the use case, and the latter doesn't
            seem to be recorded at all. So using an explicit GUID works around this, as the only critical
            point is being able to discriminate events from several instances to avoid mixing their results.
            </remarks>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1._stopwatch">
            <summary>
            Shared clock for all frame statistics.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1._lastQueuedTimeMs">
            <summary>
            Time in milliseconds since last frame was enqueued, as reported by <see cref="F:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1._stopwatch"/>.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1._lastDequeuedTimeMs">
            <summary>
            Time in milliseconds since last frame was dequeued, as reported by <see cref="F:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1._stopwatch"/>.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1._lastDroppedTimeMs">
            <summary>
            Time in milliseconds since last frame was dropped, as reported by <see cref="F:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1._stopwatch"/>.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1._queuedFrameTimeAverage">
            <summary>
            Moving average of the queued frame time, in frames per second.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1._dequeuedFrameTimeAverage">
            <summary>
            Moving average of the dequeued frame time, in frames per second.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1._droppedFrameTimeAverage">
            <summary>
            Moving average of the dropped frame time, in frames per second.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1.#ctor(System.Int32)">
            <summary>
            Create a new queue with a maximum frame length.
            </summary>
            <param name="maxQueueLength">Maxmimum number of frames to enqueue before starting to drop incoming frames</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1.Clear">
            <summary>
            Clear the queue and drop all frames currently pending.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1.Enqueue(Microsoft.MixedReality.WebRTC.I420AVideoFrame)">
            <summary>
            Enqueue a new video frame encoded in I420+Alpha format.
            If the internal queue reached its maximum capacity, do nothing and drop the frame.
            </summary>
            <param name="frame">The video frame to enqueue</param>
            <returns>Return <c>true</c> if the frame was enqueued successfully, or <c>false</c> if it was dropped</returns>
            <remarks>This should only be used if the queue has storage for a compatible video frame encoding.</remarks>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1.Enqueue(Microsoft.MixedReality.WebRTC.Argb32VideoFrame)">
            <summary>
            Try to enqueue a new video frame encoded in raw ARGB format.
            If the internal queue reached its maximum capacity, do nothing and drop the frame.
            </summary>
            <param name="frame">The video frame to enqueue</param>
            <returns>Return <c>true</c> if the frame was enqueued successfully, or <c>false</c> if it was dropped</returns>
            <remarks>This should only be used if the queue has storage for a compatible video frame encoding.</remarks>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1.TryDequeue(`0@)">
            <summary>
            Try to dequeue a video frame, usually to be consumed by a video sink (video player).
            </summary>
            <param name="frame">On success, returns the dequeued frame.</param>
            <returns>Return <c>true</c> on success or <c>false</c> if the queue is empty.</returns>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1.RecycleStorage(`0)">
            <summary>
            Recycle a frame storage, putting it back into the internal pool for later reuse.
            This prevents deallocation and reallocation of a frame, and decreases pressure on
            the garbage collector.
            </summary>
            <param name="frame">The unused frame storage to recycle for a later new frame</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1.TrackLateFrame">
            <summary>
            Track statistics for a late frame, which short-circuits the queue and is delivered
            as soon as it is received.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.VideoFrameQueue`1.GetStorageFor(System.UInt64)">
            <summary>
            Get some video frame storage for a frame of the given byte size.
            </summary>
            <param name="byteSize">The byte size of the frame that the storage should accomodate</param>
            <returns>A new or recycled storage if possible, or <c>null</c> if the queue reached its maximum capacity</returns>
        </member>
        <member name="T:Microsoft.MixedReality.WebRTC.VideoTrackSource">
             <summary>
             Video source for WebRTC video tracks.
            
             The video source is not bound to any peer connection, and can therefore be shared by multiple video
             tracks from different peer connections. This is especially useful to share local video capture devices
             (microphones) amongst multiple peer connections when building a multi-peer experience with a mesh topology
             (one connection per pair of peers).
            
             The user owns the video track source, and is in charge of keeping it alive until after all tracks using it
             are destroyed, and then dispose of it. The behavior of disposing of the track source while a track is still
             using it is undefined. The <see cref="P:Microsoft.MixedReality.WebRTC.VideoTrackSource.Tracks"/> property contains the list of tracks currently using the
             source.
             </summary>
             <seealso cref="T:Microsoft.MixedReality.WebRTC.DeviceVideoTrackSource"/>
             <seealso cref="T:Microsoft.MixedReality.WebRTC.ExternalVideoTrackSource"/>
             <seealso cref="T:Microsoft.MixedReality.WebRTC.LocalVideoTrack"/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.VideoTrackSource.Name">
            <summary>
            A name for the video track source, used for logging and debugging.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.VideoTrackSource.Tracks">
            <summary>
            List of local video tracks this source is providing raw video frames to.
            </summary>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.VideoTrackSource.FrameEncoding">
            <inheritdoc/>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.VideoTrackSource.I420AVideoFrameReady">
            <inheritdoc/>
        </member>
        <member name="E:Microsoft.MixedReality.WebRTC.VideoTrackSource.Argb32VideoFrameReady">
            <inheritdoc/>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.VideoTrackSource._nativeHandle">
            <summary>
            Handle to the native VideoTrackSource object.
            </summary>
            <remarks>
            In native land this is a <code>mrsVideoTrackSourceHandle</code>.
            </remarks>
        </member>
        <member name="P:Microsoft.MixedReality.WebRTC.VideoTrackSource.Enabled">
            <summary>
            Enabled status of the source. True until the object is disposed.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoTrackSource._selfHandle">
            <summary>
            Handle to self for interop callbacks. This adds a reference to the current object, preventing
            it from being garbage-collected.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoTrackSource._name">
            <summary>
            Backing field for <see cref="P:Microsoft.MixedReality.WebRTC.VideoTrackSource.Name"/>, and cache for the native name.
            Since the name can only be set by the user, this cached value is always up-to-date with the
            internal name of the native object, by design.
            </summary>
        </member>
        <member name="F:Microsoft.MixedReality.WebRTC.VideoTrackSource._tracks">
            <summary>
            Backing field for <see cref="P:Microsoft.MixedReality.WebRTC.VideoTrackSource.Tracks"/>.
            </summary>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.VideoTrackSource.Dispose">
            <inheritdoc/>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.VideoTrackSource.OnTrackAddedToSource(Microsoft.MixedReality.WebRTC.LocalVideoTrack)">
            <summary>
            Internal callback when a track starts using this source.
            </summary>
            <param name="track">The track using this source.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.VideoTrackSource.OnTrackRemovedFromSource(Microsoft.MixedReality.WebRTC.LocalVideoTrack)">
            <summary>
            Internal callback when a track stops using this source.
            </summary>
            <param name="track">The track not using this source anymore.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.VideoTrackSource.OnTracksRemovedFromSource(System.Collections.Generic.IEnumerable{Microsoft.MixedReality.WebRTC.LocalVideoTrack})">
            <summary>
            Internal callback when a list of tracks stop using this source, generally
            as a result of a peer connection owning said tracks being closed.
            </summary>
            <param name="tracks">The list of tracks not using this source anymore.</param>
        </member>
        <member name="M:Microsoft.MixedReality.WebRTC.VideoTrackSource.ToString">
            <inheritdoc/>
        </member>
    </members>
</doc>
